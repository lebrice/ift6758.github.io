{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import *\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_data(input_dir):\n",
    "    \"\"\"\n",
    "    Purpose: preprocess liwc and nrc\n",
    "    Input\n",
    "        input_dir {string} : path to input_directory (ex, \"~/Train\")\n",
    "    Output:\n",
    "        id_list {numpy array of strings}: array of user ids sorted alphabetically,\n",
    "                                        to determine order of features and labels DataFrames\n",
    "        text_data {pandas DataFrame of float}: unscaled text data (liwc and nrc combined)\n",
    "    \"\"\"\n",
    "    # Load and sort text data\n",
    "    liwc = pd.read_csv(os.path.join(input_dir, \"Text\", \"liwc.csv\"), sep = ',')\n",
    "    liwc = liwc.sort_values(by=['userId'])\n",
    "\n",
    "    nrc = pd.read_csv(os.path.join(input_dir, \"Text\", \"nrc.csv\"), sep = ',')\n",
    "    nrc = nrc.sort_values(by=['userId'])\n",
    "\n",
    "    # Build list of subject ids ordered alphabetically\n",
    "    # Check if same subject lists in both sorted DataFrames (liwc and nrc)\n",
    "    if np.array_equal(liwc['userId'], nrc['userId']):\n",
    "        id_list = liwc['userId'].to_numpy()\n",
    "    else:\n",
    "        raise Exception('userIds do not match between liwc and nrc data')\n",
    "\n",
    "    # merge liwc and nrc DataFrames using userId as index\n",
    "    liwc.set_index('userId', inplace=True)\n",
    "    nrc.set_index('userId', inplace=True)\n",
    "\n",
    "    text_data = pd.concat([liwc, nrc], axis=1, sort=False)\n",
    "\n",
    "    return id_list, text_data\n",
    "\n",
    "\n",
    "def get_image_clean(sub_ids, oxford, means):\n",
    "    '''\n",
    "    Purpose: preprocess oxford metrics derived from profile pictures (part 2)\n",
    "    Input:\n",
    "        sub_ids {numpy array of strings}: ordered list of userIDs\n",
    "        oxford {pandas DataFrame of floats}: unscaled oxford features of users with 1+ face\n",
    "        means {list of float}: mean values for each feature averaged from train set,\n",
    "                    to replace missing values for userids with no face (train and test set)\n",
    "    Output:\n",
    "        image_data {pandas DataFrame of float}: unscaled oxford image data\n",
    "                with mean values replacing missing entries\n",
    "    '''\n",
    "    # list of ids with at least one face on image: 7174 out of 9500 in train set\n",
    "    ox_list = np.sort(oxford['userId'].unique(), axis=None)\n",
    "    # list of ids in text_list who have no face metrics in oxford.csv (2326 in train set)\n",
    "    ox_noface = np.setdiff1d(sub_ids, ox_list)\n",
    "\n",
    "    # Create DataFrame for userids with no face (1 row per userid)\n",
    "    # values are mean metrics averaged from users with entries (training set)\n",
    "    ox_nf = pd.DataFrame(ox_noface, columns = ['userId'])\n",
    "    columns = oxford.columns[2:].tolist()\n",
    "    for column, mean in zip(columns, means):\n",
    "        ox_nf.insert(loc=ox_nf.shape[1], column=column, value=mean, allow_duplicates=True)\n",
    "    # insert column 'noface' = 1 if no face in image, else 0\n",
    "    ox_nf.insert(loc=ox_nf.shape[1], column='noface', value=1, allow_duplicates=True)\n",
    "    # insert column 'multiface' = 1 if many faces in image, else 0\n",
    "    ox_nf.insert(loc=ox_nf.shape[1], column='multiface', value=0, allow_duplicates=True)\n",
    "    ox_nf.set_index('userId', inplace=True)\n",
    "\n",
    "    # Format DataFrame from userids with 1+ face\n",
    "    # insert column 'noface' = 1 if no face in image, else 0\n",
    "    oxford.insert(loc=oxford.shape[1], column='noface', value=0, allow_duplicates=True)\n",
    "    # list userIds with multiple faces (714 in train set)\n",
    "    ox_multiples = oxford['userId'][oxford['userId'].duplicated()].tolist()\n",
    "    # insert column 'multiface' = 1 if many faces in image, else 0\n",
    "    oxford.insert(loc=oxford.shape[1], column='multiface', value=0, allow_duplicates=True)\n",
    "    multi_mask = pd.Series([uid in ox_multiples for uid in oxford['userId']])\n",
    "    i = oxford[multi_mask].index\n",
    "    oxford.loc[i, 'multiface'] = 1\n",
    "    # drop duplicate entries with same userId (keep first entry per userId)\n",
    "    oxford.drop_duplicates(subset ='userId', keep='first', inplace=True)\n",
    "\n",
    "    # merge the two DataFrames\n",
    "    oxford.drop(['faceID'], axis=1, inplace=True)\n",
    "    oxford.set_index('userId', inplace=True)\n",
    "    image_data = pd.concat([ox_nf, oxford], axis=0, sort=False).sort_values(by=['userId'])\n",
    "\n",
    "    if not np.array_equal(image_data.index, sub_ids):\n",
    "        raise Exception('userIds do not match between oxford file and id list')\n",
    "\n",
    "    return image_data\n",
    "\n",
    "\n",
    "def get_image_raw(data_dir):\n",
    "    '''\n",
    "    Purpose: preprocess oxford metrics derived from profile pictures (part 1)\n",
    "    Input\n",
    "        input_dir {string} : path to input_directory (ex, \"~/Train\")\n",
    "    Output:\n",
    "        image_data {pandas DataFrame of float}: unscaled oxford image data\n",
    "    '''\n",
    "    # Load data of oxford features extracted from profile picture (face metrics)\n",
    "    # 7915 entries; some users have no face, some have multiple faces on image.\n",
    "    # userids with 1+ face on image: 7174 out of 9500 (train set)\n",
    "    # duplicated entries (userids with > 1 face on same image): 741 in train set\n",
    "    oxford = pd.read_csv(os.path.join(data_dir, \"Image\", \"oxford.csv\"), sep = ',')\n",
    "    #oxford = oxford.sort_values(by=['userId'])\n",
    "    '''\n",
    "    NOTE: headPose_pitch has NO RANGE, drop that feature\n",
    "    '''\n",
    "    oxford.drop(['headPose_pitch'], axis=1, inplace=True)\n",
    "\n",
    "    return oxford\n",
    "\n",
    "\n",
    "def get_likes_kept(data_dir, num_features) -> List[str]:\n",
    "    '''\n",
    "    Purpose: get list of likes to keep as features\n",
    "    Input:\n",
    "        data_dir {str} : the parent input directory\n",
    "        num_features {int} : the number of likes to keep as features,\n",
    "                        starting from those with highest frequencies\n",
    "    Output:\n",
    "        freq_like_id {List of strings}: frequency of most frequent likes,\n",
    "                    (number = num_features), in descending ordered, indexed by like_id\n",
    "    '''\n",
    "    #Why return frequency?\n",
    "    relation = pd.read_csv(os.path.join(data_dir, \"Relation\", \"Relation.csv\")) #, index_col=1)\n",
    "    relation = relation.drop(['Unnamed: 0'], axis=1)\n",
    "    like_ids_to_keep = relation['like_id'].value_counts(sort=True, ascending=False)[:num_features] #This sorts features by frequency\n",
    "\n",
    "    #sort like indices (which are the keys associated with the values kepts)\n",
    "    likes_int64_list = sorted(like_ids_to_keep.keys()) # This sorts indices by like_id\n",
    "    likes_str_list = [str(l) for l in likes_int64_list]\n",
    "    return likes_str_list\n",
    "\n",
    "\n",
    "def get_relations(data_dir: str, sub_ids: List[str], like_ids_to_keep: List[str]):\n",
    "    '''\n",
    "    Purpose: preprocess relations dataset ('likes')\n",
    "\n",
    "    Input:\n",
    "        data_dir {str} -- the parent input directory\n",
    "        sub_ids {numpy array of strings} -- the ordered list of userids\n",
    "        like_ids_to_keep {List[str]} -- The list of page IDs to keep.\n",
    "\n",
    "    Returns:\n",
    "        relations_data -- multihot matrix of the like_id. Rows are indexed with userid, entries are boolean.\n",
    "    '''\n",
    "    relation = pd.read_csv(os.path.join(data_dir, \"Relation\", \"Relation.csv\")) #, index_col=1)\n",
    "    relation = relation.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    ## One HUGE step:\n",
    "    # likes_to_keep = like_ids_to_keep.keys()\n",
    "    # kept_relations = relation[relation.like_id.isin(likes_to_keep)]\n",
    "    # multi_hot_relations = pd.get_dummies(kept_relations, columns=[\"like_id\"], prefix=\"\")\n",
    "    # multi_hot = multi_hot_relations.groupby((\"userid\")).sum()\n",
    "    # return multi_hot_relations\n",
    "    ###\n",
    "    total_num_pages = len(like_ids_to_keep)\n",
    "    # Create a multihot likes matrix of booleans (rows = userids, cols = likes), by batch\n",
    "    batch_size = 1000\n",
    "\n",
    "    # Create empty DataFrame with sub_ids as index list\n",
    "    relation_data = pd.DataFrame(sub_ids, columns = ['userid'])\n",
    "    relation_data.set_index('userid', inplace=True)\n",
    "\n",
    "    for start_index in range(0, total_num_pages, batch_size):\n",
    "        end_index = min(start_index + batch_size, total_num_pages)\n",
    "\n",
    "        # sets are better for membership testing than lists.\n",
    "        like_ids_for_this_batch = set(like_ids_to_keep[start_index:end_index])\n",
    "\n",
    "        filtered_table = relation[relation['like_id'].isin(like_ids_for_this_batch)]\n",
    "        ## THIS is the slow part:\n",
    "        relHot = pd.get_dummies(filtered_table, columns=['like_id'], prefix=\"\", prefix_sep=\"\")\n",
    "        ##\n",
    "        relHot = relHot.groupby(['userid']).sum().astype(float) # this makes userid the index\n",
    "\n",
    "        relation_data = pd.concat([relation_data, relHot], axis=1, sort=True)\n",
    "\n",
    "    relation_data = relation_data.reindex(like_ids_to_keep, axis=1)\n",
    "    relation_data.fillna(0.0, inplace=True)\n",
    "    relation_data = relation_data.astype(\"bool\")\n",
    "\n",
    "    # will be different if users in relation.csv are not in sub_ids\n",
    "    if not np.array_equal(relation_data.index, sub_ids):\n",
    "        raise Exception(f\"\"\"userIds do not match between relation file and id list:\n",
    "    {relation_data.index}\n",
    "    {sub_ids}\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    return relation_data\n",
    "\n",
    "def get_likes_lists(likes_data, max_num_likes):\n",
    "    '''\n",
    "    Purpose: make list of lists of indices of liked pages per user\n",
    "    Input:\n",
    "        likes_data {pandas DataFrame}: multihot matrix of the like_id. Rows are indexed with userid, entries are boolean\n",
    "    Output:\n",
    "        lists_of_likes {list of lists of int}: indices of pages liked by each user,\n",
    "                padded with zeros to lenght = max_num_likes\n",
    "\n",
    "    '''\n",
    "    # create list of lists of indices (one per user) corresponding to liked pages in one-hot matrix\n",
    "    index_lists = []\n",
    "    for index in likes_data.index:\n",
    "        likes_indices = np.nonzero(likes_data.loc[index].to_numpy())[0].tolist()\n",
    "        index_lists.append(likes_indices)\n",
    "\n",
    "    # pad each list of indices with 0s to set lenght = max_num_likes\n",
    "    lists_padded = tf.keras.preprocessing.sequence.pad_sequences(index_lists,\n",
    "    padding='post', maxlen=max_num_likes)\n",
    "\n",
    "    lists_of_likes = pd.DataFrame(lists_padded)\n",
    "\n",
    "    lists_of_likes.insert(loc=lists_of_likes.shape[1], column='userid', value=likes_data.index, allow_duplicates=True)\n",
    "    lists_of_likes.set_index('userid', inplace=True)\n",
    "\n",
    "    return lists_of_likes\n",
    "\n",
    "def make_label_dict(labels):\n",
    "    '''\n",
    "    Purpose: make dictionnary of labels from pandas DataFrame\n",
    "    Input:\n",
    "        labels {pandas DataFrame}: labels ordered per userids (alphabetical order)\n",
    "    Output:\n",
    "        labels_dict {dictionary of pandas DataFrames}: labels (one entry per metric) ordered alphabetically\n",
    "                by userid for the training set, with userids as index.\n",
    "\n",
    "    '''\n",
    "    gender = labels['gender']\n",
    "\n",
    "    age_grps = labels[['age_xx_24', 'age_25_34', 'age_35_49', 'age_50_xx']]\n",
    "\n",
    "    '''\n",
    "    Note: : each DataFrames (value) is indexed by userid in labels_dict\n",
    "    '''\n",
    "    labels_dict = {}\n",
    "    labels_dict['userid'] = labels.index\n",
    "    labels_dict['gender'] = gender\n",
    "    labels_dict['age_grps'] = age_grps\n",
    "    labels_dict['ope'] = labels['ope']\n",
    "    labels_dict['con'] = labels['con']\n",
    "    labels_dict['ext'] = labels['ext']\n",
    "    labels_dict['agr'] = labels['agr']\n",
    "    labels_dict['neu'] = labels['neu']\n",
    "\n",
    "    return labels_dict\n",
    "\n",
    "\n",
    "def preprocess_labels(data_dir, sub_ids):\n",
    "    '''\n",
    "    Purpose: preprocess entry labels from training set\n",
    "    Input:\n",
    "        datadir {string} : path to training data directory\n",
    "        sub_ids {numpy array of strings}: list of subject ids ordered alphabetically\n",
    "    Output:\n",
    "        labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "\n",
    "    '''\n",
    "    labels = pd.read_csv(os.path.join(data_dir, \"Profile\", \"Profile.csv\"))\n",
    "\n",
    "    def age_group_id(age_str: str) -> int:\n",
    "        \"\"\"Returns the age group category ID (an integer from 0 to 3) for the given age (string)\n",
    "\n",
    "        Arguments:\n",
    "            age_str {str} -- the age\n",
    "\n",
    "        Returns:\n",
    "            int -- the ID of the age group: 0 for xx-24, 1 for 25-34, 2 for 35-49 and 3 for 50-xx.\n",
    "        \"\"\"\n",
    "        age = int(age_str)\n",
    "        if age <= 24:\n",
    "            return 0\n",
    "        elif age <= 34:\n",
    "            return 1\n",
    "        elif age <= 49:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    labels = labels.assign(age_group = lambda dt: pd.Series([age_group_id(age_str) for age_str in dt[\"age\"]]))\n",
    "    # labels = labels.assign(age_xx_24 = lambda dt: pd.Series([int(age) <= 24 for age in dt[\"age\"]]))\n",
    "    # labels = labels.assign(age_25_34 = lambda dt: pd.Series([25 <= int(age) <= 34 for age in dt[\"age\"]]))\n",
    "    # labels = labels.assign(age_35_49 = lambda dt: pd.Series([35 <= int(age) <= 49 for age in dt[\"age\"]]))\n",
    "    # labels = labels.assign(age_50_xx = lambda dt: pd.Series([50 <= int(age) for age in dt[\"age\"]]))\n",
    "\n",
    "    labels = labels.sort_values(by=['userid'])\n",
    "    # check if same subject ids in labels and sub_ids\n",
    "    if not np.array_equal(labels['userid'].to_numpy(), sub_ids):\n",
    "        raise Exception('userIds do not match between profiles labels and id list')\n",
    "\n",
    "    labels = labels.drop(['Unnamed: 0'], axis=1)\n",
    "    labels.set_index('userid', inplace=True)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def preprocess_train(data_dir, num_likes=10_000, max_num_likes=2145):\n",
    "    '''\n",
    "    Purpose: preprocesses training dataset (with labels) and returns scaled features,\n",
    "    labels and parameters to scale the test data set\n",
    "    Input\n",
    "        data_dir {string}: path to ~/Train data directory\n",
    "        num_likes {int}: number of like_ids to keep as features\n",
    "        max_num_likes {int}: maximum number of pages liked by a single user\n",
    "    Output:\n",
    "        train_features {pandas DataFrame}: vectorized features scaled between 0 and 1\n",
    "                for each user id in the training set, concatenated for all modalities\n",
    "                (order = text + image + relation), with userid as DataFrame index.\n",
    "        **(updated:)features_q10_q90 {tupple of 2 pandas Series}: series of 10th and 90th quantile values of\n",
    "                text + image features from train dataset, to be used to scale test data.\n",
    "                Note that the multihot relation features do not necessitate scaling.\n",
    "        image_means {list of float}: means from oxford dataset to replace missing entries in oxford test set\n",
    "        likes_kept {list of strings}: ordered likes_ids to serve as columns for test set relation features matrix\n",
    "        train_labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "\n",
    "    TO CONSIDER: convert outputted pandas to tensorflow tf.data.Dataset...\n",
    "    https://www.tensorflow.org/guide/data\n",
    "    '''\n",
    "    # sub_ids: a numpy array of subject ids ordered alphabetically.\n",
    "    # text_data: a pandas DataFrame of unscaled text data (liwc and nrc)\n",
    "    sub_ids, text_data = get_text_data(data_dir)\n",
    "    # image_data: pandas dataframe of oxford data\n",
    "    # image_min_max: a tupple of 2 pandas series, the min and max values from oxford training features\n",
    "    image_data_raw = get_image_raw(data_dir)\n",
    "    image_means = image_data_raw.iloc[:, 2:].mean().tolist()\n",
    "    image_data = get_image_clean(sub_ids, image_data_raw, image_means)\n",
    "\n",
    "    '''\n",
    "    Note: Scale the text and image data BEFORE concatenating with relations\n",
    "    Update: scaling w RobustScaler rather than MinMaxScaler algo, due to outliers\n",
    "    '''\n",
    "    features_to_scale = pd.concat([text_data, image_data.iloc[:, :-2]], axis=1, sort=False)\n",
    "    #feat_min = features_to_scale.min()\n",
    "    #feat_max = features_to_scale.max()\n",
    "    feat_q10 = features_to_scale.quantile(q = 0.10)\n",
    "    feat_q90 = features_to_scale.quantile(q = 0.90)\n",
    "\n",
    "    #feat_scaled = (features_to_scale - feat_min) / (feat_max - feat_min)\n",
    "    #features_min_max = (feat_min, feat_max)\n",
    "    feat_scaled = (features_to_scale - feat_q10) / (feat_q90 - feat_q10)\n",
    "    features_q10_q90 = (feat_q10, feat_q90)\n",
    "\n",
    "    likes_kept = get_likes_kept(data_dir, num_likes)\n",
    "\n",
    "    # multi-hot matrix of likes from train data\n",
    "    likes_data = get_relations(data_dir, sub_ids, likes_kept)\n",
    "\n",
    "    train_likes_lists = get_likes_lists(likes_data, max_num_likes)\n",
    "\n",
    "    # concatenate all scaled features into a single DataFrame\n",
    "    additional_weird_features = image_data.iloc[:, -2:]\n",
    "    train_features = pd.concat([feat_scaled, additional_weird_features, train_likes_lists], axis=1, sort=False)\n",
    "\n",
    "    # DataFrame of training set labels\n",
    "    train_labels = preprocess_labels(data_dir, sub_ids)\n",
    "\n",
    "\n",
    "    #return train_features, features_min_max, image_means, likes_kept, train_labels\n",
    "    return train_features, features_q10_q90, image_means, likes_kept, train_labels\n",
    "\n",
    "\n",
    "#def preprocess_test(data_dir, min_max_train, image_means_train, likes_kept_train):\n",
    "def preprocess_test(data_dir, q10_q90_train, image_means_train, likes_kept_train, max_num_likes=2145):\n",
    "    '''\n",
    "    Purpose: preprocesses test dataset (no labels)\n",
    "    Input:\n",
    "        datadir {string}: path to Test data directory\n",
    "        (**updated)q10_q90_train {tupple of two numpy arrays}: 10th and 90th quantile values for\n",
    "                concatenated text and image features (from train set)\n",
    "        image_means_train {list of float}: means from oxford training dataset to replace\n",
    "                missing entries in oxford test set\n",
    "        likes_kept_train {list of strings}: most frequent likes_ids from train set\n",
    "                (ordered by frequency) to serve as columns in relation features matrix\n",
    "        max_num_likes {int}: maximum number of pages liked by a single user (from train set)\n",
    "    Output:\n",
    "        test_features {pandas DataFrame}: vectorized features of test set\n",
    "\n",
    "    '''\n",
    "    # sub_ids: a numpy array of subject ids ordered alphabetically.\n",
    "    # text_data: a pandas DataFrame of unscaled text data (liwc and nrc)\n",
    "    sub_ids, text_data = get_text_data(data_dir)\n",
    "\n",
    "    # image_data: pandas dataframe of oxford data\n",
    "    # image_min_max: a tupple of 2 pandas series, the min and max values from oxford training features\n",
    "    image_data_raw = get_image_raw(data_dir)\n",
    "    image_data = get_image_clean(sub_ids, image_data_raw, image_means_train)\n",
    "\n",
    "    '''\n",
    "    Note: Scale the text and image data BEFORE concatenating with relations\n",
    "    '''\n",
    "    features_to_scale = pd.concat([text_data, image_data.iloc[:, :-2]], axis=1, sort=False)\n",
    "    #feat_min = min_max_train[0]\n",
    "    #feat_max = min_max_train[1]\n",
    "    feat_q10 = q10_q90_train[0]\n",
    "    feat_q90 = q10_q90_train[1]\n",
    "\n",
    "    #feat_scaled = (features_to_scale - feat_min) / (feat_max - feat_min)\n",
    "    feat_scaled = (features_to_scale - feat_q10) / (feat_q90 - feat_q10)\n",
    "\n",
    "    # multi-hot matrix of likes from train data\n",
    "    likes_data = get_relations(data_dir, sub_ids, likes_kept_train)\n",
    "\n",
    "    # list of lists of indices corresponding to pages liked\n",
    "    # each padded with 0s (list's max length = max_num_likes)\n",
    "    test_likes_lists = get_likes_lists(likes_data, max_num_likes)\n",
    "\n",
    "    # concatenate all scaled features into a single DataFrame\n",
    "    test_features = pd.concat([feat_scaled, image_data.iloc[:, -2:], test_likes_lists], axis=1, sort=False)\n",
    "\n",
    "    return test_features\n",
    "\n",
    "\n",
    "def get_train_val_sets(features, labels, val_prop):\n",
    "    '''\n",
    "    Purpose: Splits training dataset into a train and a validation set of\n",
    "    ratio determined by val_prop (x = features, y = labels)\n",
    "    Input\n",
    "        features {pandas DataFrame}: vectorized features scaled between 0 and 1\n",
    "                for each user id in the training set, concatenated for all modalities\n",
    "                (order = text + image + relation), with userid as DataFrame index.\n",
    "        labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "        val_prop {float between 0 and 1}: proportion of sample in validation set\n",
    "                    (e.g. 0.2 = 20% validation, 80% training)\n",
    "    Output:\n",
    "        x_train, x_val {pandas DataFrames}: vectorized features for train and validation sets\n",
    "        y_train, y_val {pandas DataFrames}: train and validation set labels\n",
    "\n",
    "    TO DO: convert outputted pandas to tensorflow tf.data.Dataset?...\n",
    "    https://www.tensorflow.org/guide/data\n",
    "    '''\n",
    "    # NOTE: UNUSED\n",
    "    from sklearn import model_selection\n",
    "    x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "        features, # training features to split\n",
    "        labels, # training labels to split\n",
    "        test_size = val_prop, # between 0 and 1, proportion of sample in validation set (e.g., 0.2)\n",
    "        shuffle= True,\n",
    "        #stratify = y_data[:1],\n",
    "        # random_state = 42  # can use to always obtain the same train/validation split\n",
    "        )\n",
    "\n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to preprocess the training dataset:\n",
    "# 1. set path to Train directory\n",
    "# 2. call preprocess_train\n",
    "\n",
    "train_path = '../Train' #modify if working from other directory\n",
    "\n",
    "train_features, features_q10_q90, image_means, likes_kept, train_labels = preprocess_train(train_path, num_likes=10_000, max_num_likes=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INSERT: save (as .csv) the features needed as arguments for preprocess_test in submission; \n",
    "# load in test script to feed model\n",
    "# save tensorflow models in submissions\n",
    "# https://www.tensorflow.org/guide/saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = preprocess_test(train_path, features_q10_q90, image_means, likes_kept, max_num_likes=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_features.columns == test_features.columns)/test_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test set for Age Group Classifier\n",
    "\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "    train_features, # training features to split\n",
    "    train_labels, # training labels to split\n",
    "    test_size = 0.3, # between 0 and 1, proportion of sample in validation set (e.g., 0.2)\n",
    "    shuffle= True,\n",
    "    stratify = train_labels['age_group']\n",
    "    # random_state = 42  # can use to always obtain the same train/validation split\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters model age with embedded likes\n",
    "\n",
    "num_layers=2\n",
    "dense_units=62\n",
    "learning_rate=0.0001\n",
    "l1_reg=0.0025\n",
    "l2_reg=0.005\n",
    "dropout_rate=0.1\n",
    "    \n",
    "num_text_features = 91\n",
    "num_image_features = 65 # added back noface and multiface    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.41897681451612906, 1: 0.9889946460440214, 2: 2.274281805745554, 3: 6.157407407407407}\n",
      "{0: 0.5, 1: 1.0, 2: 1.5, 3: 2.0}\n",
      "{0: 1.0, 1: 2.0, 2: 3.0, 3: 0.1}\n"
     ]
    }
   ],
   "source": [
    "# calculating weights for age categories w sklearn\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\n",
    "\n",
    "a_weights = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes = np.unique(y_train['age_group']), y= y_train['age_group'])\n",
    "\n",
    "age_weights_dict = {}\n",
    "\n",
    "for i in range(len(a_weights)):\n",
    "    age_weights_dict[i] = a_weights[i]\n",
    "    \n",
    "print(age_weights_dict)   \n",
    "\n",
    "age_weights_dict_mild = {}\n",
    "age_weights_dict_mild[0] = 0.5\n",
    "age_weights_dict_mild[1] = 1.0\n",
    "age_weights_dict_mild[2] = 1.5\n",
    "age_weights_dict_mild[3] = 2.0\n",
    "\n",
    "print(age_weights_dict_mild)\n",
    "\n",
    "age_weights_dict_first3 = {}\n",
    "age_weights_dict_first3[0] = 1.0\n",
    "age_weights_dict_first3[1] = 2.0\n",
    "age_weights_dict_first3[2] = 3.0\n",
    "age_weights_dict_first3[3] = 0.1\n",
    "\n",
    "print(age_weights_dict_first3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "likes_features (InputLayer)     [(None, 2000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_features (InputLayer)      [(None, 91)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_features (InputLayer)     [(None, 65)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "likes_embedding_block (Sequenti (None, 16000)        80000       likes_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_layers (Sequential)       (None, 62)           1005640     text_features[0][0]              \n",
      "                                                                 image_features[0][0]             \n",
      "                                                                 likes_embedding_block[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "age_group (Dense)               (None, 4)            252         dense_layers[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,085,892\n",
      "Trainable params: 1,085,892\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model category age using embedding for likes\n",
    "\n",
    "max_len = 2000\n",
    "\n",
    "image_features = tf.keras.Input([num_image_features], dtype=tf.float32, name=\"image_features\")\n",
    "text_features  = tf.keras.Input([num_text_features], dtype=tf.float32, name=\"text_features\")\n",
    "likes_features = tf.keras.Input([max_len], dtype=tf.int32, name=\"likes_features\")\n",
    "\n",
    "likes_embedding_block = tf.keras.Sequential(name=\"likes_embedding_block\")\n",
    "likes_embedding_block.add(tf.keras.layers.Embedding(10000, 8, input_length=max_len,\n",
    "                                                   mask_zero=True))\n",
    "\n",
    "likes_embedding_block.add(tf.keras.layers.Flatten())\n",
    "\n",
    "condensed_likes = likes_embedding_block(likes_features)\n",
    "\n",
    "dense_layers = tf.keras.Sequential(name=\"dense_layers\")\n",
    "dense_layers.add(tf.keras.layers.Concatenate())\n",
    "for i in range(num_layers):\n",
    "    dense_layers.add(tf.keras.layers.Dense(\n",
    "        units=dense_units,\n",
    "        activation= 'tanh', #'tanh',\n",
    "        kernel_regularizer=tf.keras.regularizers.L1L2(l1=l1_reg, l2=l2_reg),      \n",
    "        ))\n",
    "        \n",
    "    dense_layers.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "features = dense_layers([text_features, image_features, condensed_likes])\n",
    "\n",
    "age_group = tf.keras.layers.Dense(units=4, activation=\"softmax\", name=\"age_group\")(features)\n",
    "\n",
    "model_age = tf.keras.Model(\n",
    "    inputs=[text_features, image_features, likes_features],\n",
    "    outputs= age_group\n",
    ")    \n",
    "\n",
    "model_age.compile(\n",
    "    optimizer = tf.keras.optimizers.get({\"class_name\": \"ADAM\", #'ADAM'\n",
    "                               \"config\": {\"learning_rate\": 0.0001}}),    \n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['acc', 'categorical_accuracy']\n",
    ")\n",
    "\n",
    "print(model_age.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x_train_txt = x_train.iloc[:, :91].values\n",
    "x_train_img = x_train.iloc[:, 91:156].values\n",
    "x_train_lik = x_train.iloc[:, 156:].values\n",
    "\n",
    "x_val_txt = x_val.iloc[:, :91].values\n",
    "x_val_img = x_val.iloc[:, 91:156].values\n",
    "x_val_lik = x_val.iloc[:, 156:].values\n",
    "\n",
    "y_train_age = tf.keras.utils.to_categorical(y_train['age_group'].values)\n",
    "\n",
    "y_val_age = tf.keras.utils.to_categorical(y_val['age_group'].values)\n",
    "\n",
    "'''\n",
    "\n",
    "#Train on entire the train set (100%)\n",
    "\n",
    "x_train_txt = train_features.iloc[:, :91].values\n",
    "x_train_img = train_features.iloc[:, 91:156].values\n",
    "x_train_lik = train_features.iloc[:, 156:].values\n",
    "\n",
    "y_train_age = tf.keras.utils.to_categorical(train_labels['age_group'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9500 samples\n",
      "Epoch 1/55\n",
      "9500/9500 [==============================] - 3s 286us/sample - loss: 14.4379 - acc: 0.5174 - categorical_accuracy: 0.5174\n",
      "Epoch 2/55\n",
      "9500/9500 [==============================] - 2s 195us/sample - loss: 3.2918 - acc: 0.5659 - categorical_accuracy: 0.5659\n",
      "Epoch 3/55\n",
      "9500/9500 [==============================] - 2s 192us/sample - loss: 2.7922 - acc: 0.5729 - categorical_accuracy: 0.5729\n",
      "Epoch 4/55\n",
      "9500/9500 [==============================] - 2s 192us/sample - loss: 2.6611 - acc: 0.5681 - categorical_accuracy: 0.5681\n",
      "Epoch 5/55\n",
      "9500/9500 [==============================] - 2s 191us/sample - loss: 2.5454 - acc: 0.5792 - categorical_accuracy: 0.5792\n",
      "Epoch 6/55\n",
      "9500/9500 [==============================] - 2s 193us/sample - loss: 2.4457 - acc: 0.5819 - categorical_accuracy: 0.5819\n",
      "Epoch 7/55\n",
      "9500/9500 [==============================] - 2s 191us/sample - loss: 2.3606 - acc: 0.5776 - categorical_accuracy: 0.5776\n",
      "Epoch 8/55\n",
      "9500/9500 [==============================] - 2s 192us/sample - loss: 2.2806 - acc: 0.5865 - categorical_accuracy: 0.5865\n",
      "Epoch 9/55\n",
      "9500/9500 [==============================] - 2s 191us/sample - loss: 2.2016 - acc: 0.5971 - categorical_accuracy: 0.5971\n",
      "Epoch 10/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 2.1277 - acc: 0.5982 - categorical_accuracy: 0.5982\n",
      "Epoch 11/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 2.0546 - acc: 0.5977 - categorical_accuracy: 0.5977\n",
      "Epoch 12/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.9938 - acc: 0.5938 - categorical_accuracy: 0.5938\n",
      "Epoch 13/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.9313 - acc: 0.5944 - categorical_accuracy: 0.5944\n",
      "Epoch 14/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.8839 - acc: 0.5949 - categorical_accuracy: 0.5949\n",
      "Epoch 15/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.8294 - acc: 0.6007 - categorical_accuracy: 0.6007\n",
      "Epoch 16/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.7834 - acc: 0.5978 - categorical_accuracy: 0.5978\n",
      "Epoch 17/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.7379 - acc: 0.6022 - categorical_accuracy: 0.6022\n",
      "Epoch 18/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.6952 - acc: 0.6062 - categorical_accuracy: 0.6062\n",
      "Epoch 19/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.6621 - acc: 0.6151 - categorical_accuracy: 0.6151\n",
      "Epoch 20/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.6237 - acc: 0.6223 - categorical_accuracy: 0.6223\n",
      "Epoch 21/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.5898 - acc: 0.6273 - categorical_accuracy: 0.6273\n",
      "Epoch 22/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.5530 - acc: 0.6375 - categorical_accuracy: 0.6375\n",
      "Epoch 23/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.5185 - acc: 0.6463 - categorical_accuracy: 0.6463\n",
      "Epoch 24/55\n",
      "9500/9500 [==============================] - 2s 193us/sample - loss: 1.4832 - acc: 0.6526 - categorical_accuracy: 0.6526\n",
      "Epoch 25/55\n",
      "9500/9500 [==============================] - 2s 194us/sample - loss: 1.4512 - acc: 0.6624 - categorical_accuracy: 0.6624\n",
      "Epoch 26/55\n",
      "9500/9500 [==============================] - 2s 193us/sample - loss: 1.4250 - acc: 0.6688 - categorical_accuracy: 0.6688\n",
      "Epoch 27/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.3942 - acc: 0.6804 - categorical_accuracy: 0.6804\n",
      "Epoch 28/55\n",
      "9500/9500 [==============================] - 2s 189us/sample - loss: 1.3710 - acc: 0.6849 - categorical_accuracy: 0.6849\n",
      "Epoch 29/55\n",
      "9500/9500 [==============================] - 2s 189us/sample - loss: 1.3484 - acc: 0.6867 - categorical_accuracy: 0.6867\n",
      "Epoch 30/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.3258 - acc: 0.6893 - categorical_accuracy: 0.6893\n",
      "Epoch 31/55\n",
      "9500/9500 [==============================] - 2s 189us/sample - loss: 1.3070 - acc: 0.6969 - categorical_accuracy: 0.6969\n",
      "Epoch 32/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.2889 - acc: 0.6955 - categorical_accuracy: 0.6955\n",
      "Epoch 33/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.2758 - acc: 0.6984 - categorical_accuracy: 0.6984\n",
      "Epoch 34/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.2613 - acc: 0.7021 - categorical_accuracy: 0.7021\n",
      "Epoch 35/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.2460 - acc: 0.7063 - categorical_accuracy: 0.7063\n",
      "Epoch 36/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.2352 - acc: 0.7006 - categorical_accuracy: 0.7006\n",
      "Epoch 37/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.2280 - acc: 0.7062 - categorical_accuracy: 0.7062\n",
      "Epoch 38/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.2101 - acc: 0.7125 - categorical_accuracy: 0.7125\n",
      "Epoch 39/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.1926 - acc: 0.7133 - categorical_accuracy: 0.7133\n",
      "Epoch 40/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.1866 - acc: 0.7133 - categorical_accuracy: 0.7133\n",
      "Epoch 41/55\n",
      "9500/9500 [==============================] - 2s 191us/sample - loss: 1.1777 - acc: 0.7172 - categorical_accuracy: 0.7172\n",
      "Epoch 42/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.1678 - acc: 0.7202 - categorical_accuracy: 0.7202\n",
      "Epoch 43/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.1527 - acc: 0.7252 - categorical_accuracy: 0.7252\n",
      "Epoch 44/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.1451 - acc: 0.7269 - categorical_accuracy: 0.7269\n",
      "Epoch 45/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.1300 - acc: 0.7299 - categorical_accuracy: 0.7299\n",
      "Epoch 46/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.1093 - acc: 0.7356 - categorical_accuracy: 0.7356\n",
      "Epoch 47/55\n",
      "9500/9500 [==============================] - 2s 189us/sample - loss: 1.1040 - acc: 0.7420 - categorical_accuracy: 0.7420\n",
      "Epoch 48/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.0914 - acc: 0.7413 - categorical_accuracy: 0.7413\n",
      "Epoch 49/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.0808 - acc: 0.7495 - categorical_accuracy: 0.7495\n",
      "Epoch 50/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.0660 - acc: 0.7526 - categorical_accuracy: 0.7526\n",
      "Epoch 51/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.0492 - acc: 0.7577 - categorical_accuracy: 0.7577\n",
      "Epoch 52/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.0367 - acc: 0.7638 - categorical_accuracy: 0.7638\n",
      "Epoch 53/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.0240 - acc: 0.7674 - categorical_accuracy: 0.7674\n",
      "Epoch 54/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 1.0077 - acc: 0.7735 - categorical_accuracy: 0.7735\n",
      "Epoch 55/55\n",
      "9500/9500 [==============================] - 2s 190us/sample - loss: 0.9942 - acc: 0.7803 - categorical_accuracy: 0.7803\n",
      "0.5967368421052631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4761,  900,    8,    0],\n",
       "       [ 163, 1997,  241,    0],\n",
       "       [   2,  310,  733,    0],\n",
       "       [  36,  163,  186,    0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "history_age_balanced_mild = model_age.fit([x_train_txt, x_train_img, x_train_lik], y_train_age, \n",
    "                                     shuffle=True, batch_size=64, epochs=70, verbose=1,\n",
    "                                     validation_data=([x_val_txt, x_val_img, x_val_lik], y_val_age),\n",
    "                                     class_weight=age_weights_dict_first3\n",
    "                                    )\n",
    "\n",
    "'''\n",
    "\n",
    "history_age_fullset = model_age.fit([x_train_txt, x_train_img, x_train_lik], y_train_age, \n",
    "                                     shuffle=True, batch_size=64, epochs=55, verbose=1,\n",
    "                                     #validation_data=([x_val_txt, x_val_img, x_val_lik], y_val_age),\n",
    "                                     class_weight=age_weights_dict_first3\n",
    "                                    )\n",
    "\n",
    "\n",
    "# validation baseline for age group:\n",
    "#print(y_val['age_group'].value_counts()[0]/y_val.shape[0])\n",
    "print(train_labels['age_group'].value_counts()[0]/train_labels.shape[0])\n",
    "\n",
    "## Create confusion matrix\n",
    "# 0 = < 25\n",
    "# 1 = 24-35\n",
    "# 2 = 35-50\n",
    "# 3 = 50+\n",
    "\n",
    "'''\n",
    "y_pred = np.argmax(model_age.predict([x_val_txt, x_val_img, x_val_lik]), axis=1)\n",
    "y_true = np.argmax(y_val_age, axis=1)\n",
    "\n",
    "cm = sklearn.metrics.confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n",
    "print(cm)\n",
    "'''\n",
    "\n",
    "y_pred = np.argmax(model_age.predict([x_train_txt, x_train_img, x_train_lik]), axis=1)\n",
    "y_true = np.argmax(y_train_age, axis=1)\n",
    "\n",
    "cm = sklearn.metrics.confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_age.save('saved_models/age_model_embedding_2000_fullset.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training notes: \n",
    "# Without rebalancing of weights, at learning rate 0.0001, starts overfitting (loss increasing) \n",
    "# around epoch 70\n",
    "\n",
    "# More balanced classification: batches 16, 30 epochs, reweight 1, 2, 3, 0.1\n",
    "# For 64 size batches, loss increases around 55... still good classification around 70\n",
    "\n",
    "## Better training: 64 batches, 55 epochs, reweight 1, 2, 3, 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### try to recreate same model from saved weights\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "def get_age_model() -> tf.keras.Model:\n",
    "    ###CHANGE THIS TO FULL SET!\n",
    "    ### Dans model.py: change best_model_so_far to \n",
    "    # checkpoints/one-model-each-marie-3/2019-11-26_17-57-56 \n",
    "    age_model_path = 'saved_models/age_model_embedding_2000_fullset.h5'\n",
    "\n",
    "    num_layers=2\n",
    "    ## CHANGE TO 62!!\n",
    "    dense_units=62\n",
    "    learning_rate=0.00005\n",
    "    l1_reg=0.0025\n",
    "    l2_reg=0.005\n",
    "    dropout_rate=0.1\n",
    "    num_text_features = 91\n",
    "    num_image_features = 65\n",
    "    max_len = 2000\n",
    "\n",
    "    image_features = tf.keras.Input([num_image_features], dtype=tf.float32, name=\"image_features\")\n",
    "    text_features  = tf.keras.Input([num_text_features], dtype=tf.float32, name=\"text_features\")\n",
    "    likes_features = tf.keras.Input([max_len], dtype=tf.int32, name=\"likes_features\")\n",
    "    \n",
    "    likes_embedding_block = tf.keras.Sequential(name=\"likes_embedding_block\")\n",
    "    ### ADD: mask_zero = True\n",
    "    likes_embedding_block.add(tf.keras.layers.Embedding(10000, 8, input_length=max_len,\n",
    "                                                       mask_zero=True))\n",
    "    \n",
    "    likes_embedding_block.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    condensed_likes = likes_embedding_block(likes_features)\n",
    "    \n",
    "    dense_layers = tf.keras.Sequential(name=\"dense_layers\")\n",
    "    dense_layers.add(tf.keras.layers.Concatenate())\n",
    "    for i in range(num_layers):\n",
    "        dense_layers.add(tf.keras.layers.Dense(\n",
    "            units=dense_units,\n",
    "            activation= 'tanh', #'tanh',\n",
    "            kernel_regularizer=tf.keras.regularizers.L1L2(l1=l1_reg, l2=l2_reg),\n",
    "            ))\n",
    "\n",
    "        dense_layers.add(tf.keras.layers.Dropout(dropout_rate))        \n",
    "\n",
    "    features = dense_layers([text_features, image_features, condensed_likes])\n",
    "\n",
    "    age_group = tf.keras.layers.Dense(units=4, activation=\"softmax\", name=\"age_group\")(features)\n",
    "\n",
    "    model_age = tf.keras.Model(\n",
    "        inputs=[text_features, image_features, likes_features],\n",
    "        outputs= age_group\n",
    "    )\n",
    "    \n",
    "    model_age.compile(\n",
    "        ##change learning rate\n",
    "        optimizer = tf.keras.optimizers.get({\"class_name\": 'ADAM',\n",
    "                                   \"config\": {\"learning_rate\": 0.0001}}),\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics = ['acc', 'categorical_accuracy']\n",
    "    )\n",
    "\n",
    "    model_age.load_weights(age_model_path)\n",
    "\n",
    "    return model_age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saved_age_model = get_age_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4708,  947,   14,    0],\n",
       "       [ 455, 1725,  221,    0],\n",
       "       [  12,  380,  653,    0],\n",
       "       [  33,  148,  204,    0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(model_age.predict([x_train_txt, x_train_img, x_train_lik]), axis=1)\n",
    "y_true = np.argmax(y_train_age, axis=1)\n",
    "\n",
    "cm = sklearn.metrics.confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7458947368421053"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred==y_true)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4708,  947,   14,    0],\n",
       "       [ 455, 1725,  221,    0],\n",
       "       [  12,  380,  653,    0],\n",
       "       [  33,  148,  204,    0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(saved_age_model.predict([x_train_txt, x_train_img, x_train_lik]), axis=1)\n",
    "y_true = np.argmax(y_train_age, axis=1)\n",
    "\n",
    "cm = sklearn.metrics.confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3])\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7458947368421053"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred==y_true)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
