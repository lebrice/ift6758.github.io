{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import *\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is a slightly modified version of the code from the preprocessing_pipeline.py \n",
    "# script that can stand alone in a notebook\n",
    "\n",
    "# to skip scaling... \n",
    "\n",
    "def get_text_data(input_dir):\n",
    "    \"\"\"\n",
    "    Purpose: preprocess liwc and nrc\n",
    "    Input\n",
    "        input_dir {string} : path to input_directory (ex, \"~/Train\")\n",
    "    Output:\n",
    "        id_list {numpy array of strings}: array of user ids sorted alphabetically,\n",
    "                                        to determine order of features and labels DataFrames\n",
    "        text_data {pandas DataFrame of float}: unscaled text data (liwc and nrc combined)\n",
    "    \"\"\"\n",
    "    # Load and sort text data\n",
    "    liwc = pd.read_csv(os.path.join(input_dir, 'Text/liwc.csv'), sep = ',')\n",
    "    liwc = liwc.sort_values(by=['userId'])\n",
    "\n",
    "    nrc = pd.read_csv(os.path.join(input_dir, 'Text/nrc.csv'), sep = ',')\n",
    "    nrc = nrc.sort_values(by=['userId'])\n",
    "\n",
    "    # Build list of subject ids ordered alphabetically\n",
    "    # Check if same subject lists in both sorted DataFrames (liwc and nrc)\n",
    "    if np.array_equal(liwc['userId'], nrc['userId']):\n",
    "        id_list = liwc['userId'].to_numpy()\n",
    "    else:\n",
    "        raise Exception('userIds do not match between liwc and nrc data')\n",
    "\n",
    "    # merge liwc and nrc DataFrames using userId as index\n",
    "    liwc.set_index('userId', inplace=True)\n",
    "    nrc.set_index('userId', inplace=True)\n",
    "\n",
    "    text_data = pd.concat([liwc, nrc], axis=1, sort=False)\n",
    "\n",
    "    return id_list, text_data\n",
    "\n",
    "\n",
    "def get_image_clean(sub_ids, oxford, means):\n",
    "    '''\n",
    "    Purpose: preprocess oxford metrics derived from profile pictures (part 2)\n",
    "    Input:\n",
    "        sub_ids {numpy array of strings}: ordered list of userIDs\n",
    "        oxford {pandas DataFrame of floats}: unscaled oxford features of users with 1+ face\n",
    "        means {list of float}: mean values for each feature averaged from train set,\n",
    "                    to replace missing values for userids with no face (train and test set)\n",
    "    Output:\n",
    "        image_data {pandas DataFrame of float}: unscaled oxford image data\n",
    "                with mean values replacing missing entries\n",
    "    '''\n",
    "    # list of ids with at least one face on image: 7174 out of 9500 in train set\n",
    "    ox_list = np.sort(oxford['userId'].unique(), axis=None)\n",
    "    # list of ids in text_list who have no face metrics in oxford.csv (2326 in train set)\n",
    "    ox_noface = np.setdiff1d(sub_ids, ox_list)\n",
    "\n",
    "    # Create DataFrame for userids with no face (1 row per userid)\n",
    "    # values are mean metrics averaged from users with entries (training set)\n",
    "    ox_nf = pd.DataFrame(ox_noface, columns = ['userId'])\n",
    "    columns = oxford.columns[2:].tolist()\n",
    "    for column, mean in zip(columns, means):\n",
    "        ox_nf.insert(loc=ox_nf.shape[1], column=column, value=mean, allow_duplicates=True)\n",
    "    # insert column 'noface' = 1 if no face in image, else 0\n",
    "    ox_nf.insert(loc=ox_nf.shape[1], column='noface', value=1, allow_duplicates=True)\n",
    "    # insert column 'multiface' = 1 if many faces in image, else 0\n",
    "    ox_nf.insert(loc=ox_nf.shape[1], column='multiface', value=0, allow_duplicates=True)\n",
    "    ox_nf.set_index('userId', inplace=True)\n",
    "\n",
    "    # Format DataFrame from userids with 1+ face\n",
    "    # insert column 'noface' = 1 if no face in image, else 0\n",
    "    oxford.insert(loc=oxford.shape[1], column='noface', value=0, allow_duplicates=True)\n",
    "    # list userIds with multiple faces (714 in train set)\n",
    "    ox_multiples = oxford['userId'][oxford['userId'].duplicated()].tolist()\n",
    "    # insert column 'multiface' = 1 if many faces in image, else 0\n",
    "    oxford.insert(loc=oxford.shape[1], column='multiface', value=0, allow_duplicates=True)\n",
    "    multi_mask = pd.Series([uid in ox_multiples for uid in oxford['userId']])\n",
    "    i = oxford[multi_mask].index\n",
    "    oxford.loc[i, 'multiface'] = 1\n",
    "    # drop duplicate entries with same userId (keep first entry per userId)\n",
    "    oxford.drop_duplicates(subset ='userId', keep='first', inplace=True)\n",
    "\n",
    "    # merge the two DataFrames\n",
    "    oxford.drop(['faceID'], axis=1, inplace=True)\n",
    "    oxford.set_index('userId', inplace=True)\n",
    "    image_data = pd.concat([ox_nf, oxford], axis=0, sort=False).sort_values(by=['userId'])\n",
    "\n",
    "    if not np.array_equal(image_data.index, sub_ids):\n",
    "        raise Exception('userIds do not match between oxford file and id list')\n",
    "\n",
    "    return image_data\n",
    "\n",
    "\n",
    "def get_image_raw(data_dir):\n",
    "    '''\n",
    "    Purpose: preprocess oxford metrics derived from profile pictures (part 1)\n",
    "    Input\n",
    "        input_dir {string} : path to input_directory (ex, \"~/Train\")\n",
    "    Output:\n",
    "        image_data {pandas DataFrame of float}: unscaled oxford image data\n",
    "    '''\n",
    "    # Load data of oxford features extracted from profile picture (face metrics)\n",
    "    # 7915 entries; some users have no face, some have multiple faces on image.\n",
    "    # userids with 1+ face on image: 7174 out of 9500 (train set)\n",
    "    # duplicated entries (userids with > 1 face on same image): 741 in train set\n",
    "    oxford = pd.read_csv(os.path.join(data_dir, \"Image\", \"oxford.csv\"), sep = ',')\n",
    "    oxford = oxford.sort_values(by=['userId'])\n",
    "    '''\n",
    "    NOTE: headPose_pitch has NO RANGE, drop that feature\n",
    "    '''\n",
    "    oxford.drop(['headPose_pitch'], axis=1, inplace=True)\n",
    "\n",
    "    return oxford\n",
    "\n",
    "\n",
    "def get_likes_kept(data_dir, num_features) -> List[str]:\n",
    "    '''\n",
    "    Purpose: get list of likes to keep as features\n",
    "    Input:\n",
    "        data_dir {str} : the parent input directory\n",
    "        num_features {int} : the number of likes to keep as features,\n",
    "                        starting from those with highest frequencies\n",
    "    Output:\n",
    "        freq_like_id {List of strings}: frequency of most frequent likes,\n",
    "                    (number = num_features), in descending ordered, indexed by like_id\n",
    "    '''\n",
    "    #Why return frequency?\n",
    "    relation = pd.read_csv(os.path.join(data_dir, \"Relation\", \"Relation.csv\")) #, index_col=1)\n",
    "    relation = relation.drop(['Unnamed: 0'], axis=1)\n",
    "    like_ids_to_keep = relation['like_id'].value_counts(sort=True, ascending=False)[:num_features] #This sorts features by frequency\n",
    "\n",
    "    #sort like indices (which are the keys associated with the values kepts)\n",
    "    likes_int64_list = sorted(like_ids_to_keep.keys()) # This sorts indices by like_id\n",
    "    likes_str_list = [str(l) for l in likes_int64_list]\n",
    "    return likes_str_list\n",
    "\n",
    "\n",
    "def get_relations(data_dir: str, sub_ids: List[str], like_ids_to_keep: List[str]):\n",
    "    '''\n",
    "    Purpose: preprocess relations dataset ('likes')\n",
    "\n",
    "    Input:\n",
    "        data_dir {str} -- the parent input directory\n",
    "        sub_ids {numpy array of strings} -- the ordered list of userids\n",
    "        like_ids_to_keep {List[str]} -- The list of page IDs to keep.\n",
    "\n",
    "    Returns:\n",
    "        relations_data -- multihot matrix of the like_id. Rows are indexed with userid, entries are boolean.\n",
    "    '''\n",
    "    relation = pd.read_csv(os.path.join(data_dir, \"Relation\", \"Relation.csv\")) #, index_col=1)\n",
    "    relation = relation.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    ## One HUGE step:\n",
    "    # likes_to_keep = like_ids_to_keep.keys()\n",
    "    # kept_relations = relation[relation.like_id.isin(likes_to_keep)]\n",
    "    # multi_hot_relations = pd.get_dummies(kept_relations, columns=[\"like_id\"], prefix=\"\")\n",
    "    # multi_hot = multi_hot_relations.groupby((\"userid\")).sum()\n",
    "    # return multi_hot_relations\n",
    "    ###\n",
    "    total_num_pages = len(like_ids_to_keep)\n",
    "    # Create a multihot likes matrix of booleans (rows = userids, cols = likes), by batch\n",
    "    batch_size = 1000\n",
    "\n",
    "    # Create empty DataFrame with sub_ids as index list\n",
    "    relation_data = pd.DataFrame(sub_ids, columns = ['userid'])\n",
    "    relation_data.set_index('userid', inplace=True)\n",
    "\n",
    "    for start_index in range(0, total_num_pages, batch_size):\n",
    "        end_index = min(start_index + batch_size, total_num_pages)\n",
    "\n",
    "        # sets are better for membership testing than lists.\n",
    "        like_ids_for_this_batch = set(like_ids_to_keep[start_index:end_index])\n",
    "\n",
    "        filtered_table = relation[relation['like_id'].isin(like_ids_for_this_batch)]\n",
    "        ## THIS is the slow part:\n",
    "        relHot = pd.get_dummies(filtered_table, columns=['like_id'], prefix=\"\", prefix_sep=\"\")\n",
    "        ##\n",
    "        relHot = relHot.groupby(['userid']).sum().astype(float) # this makes userid the index\n",
    "\n",
    "        relation_data = pd.concat([relation_data, relHot], axis=1, sort=True)\n",
    "\n",
    "    relation_data = relation_data.reindex(like_ids_to_keep, axis=1)\n",
    "    relation_data.fillna(0.0, inplace=True)\n",
    "    relation_data = relation_data.astype(\"bool\")\n",
    "\n",
    "    # will be different if users in relation.csv are not in sub_ids\n",
    "    if not np.array_equal(relation_data.index, sub_ids):\n",
    "        raise Exception(f\"\"\"userIds do not match between relation file and id list:\n",
    "    {relation_data.index}\n",
    "    {sub_ids}\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    return relation_data\n",
    "\n",
    "\n",
    "def make_label_dict(labels):\n",
    "    '''\n",
    "    Purpose: make dictionnary of labels from pandas DataFrame\n",
    "    Input:\n",
    "        labels {pandas DataFrame}: labels ordered per userids (alphabetical order)\n",
    "    Output:\n",
    "        labels_dict {dictionary of pandas DataFrames}: labels (one entry per metric) ordered alphabetically\n",
    "                by userid for the training set, with userids as index.\n",
    "\n",
    "    '''\n",
    "    gender = labels['gender']\n",
    "\n",
    "    age_grps = labels[['age_xx_24', 'age_25_34', 'age_35_49', 'age_50_xx']]\n",
    "\n",
    "    '''\n",
    "    Note: : each DataFrames (value) is indexed by userid in labels_dict\n",
    "    '''\n",
    "    labels_dict = {}\n",
    "    labels_dict['userid'] = labels.index\n",
    "    labels_dict['gender'] = gender\n",
    "    labels_dict['age_grps'] = age_grps\n",
    "    labels_dict['ope'] = labels['ope']\n",
    "    labels_dict['con'] = labels['con']\n",
    "    labels_dict['ext'] = labels['ext']\n",
    "    labels_dict['agr'] = labels['agr']\n",
    "    labels_dict['neu'] = labels['neu']\n",
    "\n",
    "    return labels_dict\n",
    "\n",
    "\n",
    "def preprocess_labels(data_dir, sub_ids):\n",
    "    '''\n",
    "    Purpose: preprocess entry labels from training set\n",
    "    Input:\n",
    "        datadir {string} : path to training data directory\n",
    "        sub_ids {numpy array of strings}: list of subject ids ordered alphabetically\n",
    "    Output:\n",
    "        labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "\n",
    "    '''\n",
    "    labels = pd.read_csv(os.path.join(data_dir, \"Profile\", \"Profile.csv\"))\n",
    "    labels = labels.sort_values(by=['userid'])\n",
    "    # check if same subject ids in labels and sub_ids\n",
    "    if not np.array_equal(labels['userid'].to_numpy(), sub_ids):\n",
    "        raise Exception('userIds do not match between profiles labels and id list')\n",
    "\n",
    "    def age_group_id(age_str: str) -> int:\n",
    "        \"\"\"Returns the age group category ID (an integer from 0 to 3) for the given age (string)\n",
    "\n",
    "        Arguments:\n",
    "            age_str {str} -- the age\n",
    "\n",
    "        Returns:\n",
    "            int -- the ID of the age group: 0 for xx-24, 1 for 25-34, 2 for 35-49 and 3 for 50-xx.\n",
    "        \"\"\"\n",
    "        age = int(age_str)\n",
    "        if age <= 24:\n",
    "            return 0\n",
    "        elif age <= 34:\n",
    "            return 1\n",
    "        elif age <= 49:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    labels = labels.assign(age_group = lambda dt: pd.Series([age_group_id(age_str) for age_str in dt[\"age\"]]))\n",
    "    # labels = labels.assign(age_xx_24 = lambda dt: pd.Series([int(age) <= 24 for age in dt[\"age\"]]))\n",
    "    # labels = labels.assign(age_25_34 = lambda dt: pd.Series([25 <= int(age) <= 34 for age in dt[\"age\"]]))\n",
    "    # labels = labels.assign(age_35_49 = lambda dt: pd.Series([35 <= int(age) <= 49 for age in dt[\"age\"]]))\n",
    "    # labels = labels.assign(age_50_xx = lambda dt: pd.Series([50 <= int(age) for age in dt[\"age\"]]))\n",
    "\n",
    "    labels = labels.drop(['Unnamed: 0'], axis=1)\n",
    "    labels.set_index('userid', inplace=True)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def preprocess_train(data_dir, num_likes=10_000, scaling=True):\n",
    "    '''\n",
    "    Purpose: preprocesses training dataset (with labels) and returns scaled features,\n",
    "    labels and parameters to scale the test data set\n",
    "    Input\n",
    "        data_dir {string}: path to ~/Train data directory\n",
    "        num_likes {int}: number of like_ids to keep as features\n",
    "        scaling {boolean}: if True, Robust scaling applied to data; no scaling if False                \n",
    "    Output:\n",
    "        train_features {pandas DataFrame}: vectorized features scaled between 0 and 1\n",
    "                for each user id in the training set, concatenated for all modalities\n",
    "                (order = text + image + relation), with userid as DataFrame index.\n",
    "        **(updated:)features_q10_q90 {tupple of 2 pandas Series}: series of 10th and 90th quantile values of\n",
    "                text + image features from train dataset, to be used to scale test data.\n",
    "                Note that the multihot relation features do not necessitate scaling.\n",
    "        image_means {list of float}: means from oxford dataset to replace missing entries in oxford test set\n",
    "        likes_kept {list of strings}: ordered likes_ids to serve as columns for test set relation features matrix\n",
    "        train_labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "\n",
    "    TO CONSIDER: convert outputted pandas to tensorflow tf.data.Dataset...\n",
    "    https://www.tensorflow.org/guide/data\n",
    "    '''\n",
    "    # sub_ids: a numpy array of subject ids ordered alphabetically.\n",
    "    # text_data: a pandas DataFrame of unscaled text data (liwc and nrc)\n",
    "    sub_ids, text_data = get_text_data(data_dir)\n",
    "    # image_data: pandas dataframe of oxford data\n",
    "    # image_min_max: a tupple of 2 pandas series, the min and max values from oxford training features\n",
    "    image_data_raw = get_image_raw(data_dir)\n",
    "    image_means = image_data_raw.iloc[:, 2:].mean().tolist()\n",
    "    image_data = get_image_clean(sub_ids, image_data_raw, image_means)\n",
    "\n",
    "    '''\n",
    "    Note: Scale the text and image data BEFORE concatenating with relations\n",
    "    Update: scaling w RobustScaler rather than MinMaxScaler algo, due to outliers\n",
    "    '''\n",
    "    features_to_scale = pd.concat([text_data, image_data.iloc[:, :-2]], axis=1, sort=False)\n",
    "    #feat_min = features_to_scale.min()\n",
    "    #feat_max = features_to_scale.max()\n",
    "    feat_q10 = features_to_scale.quantile(q = 0.10)\n",
    "    feat_q90 = features_to_scale.quantile(q = 0.90)\n",
    "\n",
    "    #feat_scaled = (features_to_scale - feat_min) / (feat_max - feat_min)\n",
    "    #features_min_max = (feat_min, feat_max)\n",
    "    if scaling:\n",
    "        feat_scaled = (features_to_scale - feat_q10) / (feat_q90 - feat_q10)\n",
    "    else:\n",
    "        feat_scaled = features_to_scale\n",
    "    features_q10_q90 = (feat_q10, feat_q90)\n",
    "\n",
    "    #if DEBUG:\n",
    "    #    likes_kept = [str(v) for v in range(num_likes)]\n",
    "    #else:\n",
    "    likes_kept = get_likes_kept(data_dir, num_likes)\n",
    "\n",
    "    # multi-hot matrix of likes from train data\n",
    "    likes_data = get_relations(data_dir, sub_ids, likes_kept)\n",
    "\n",
    "    # concatenate all scaled features into a single DataFrame\n",
    "    train_features = pd.concat([feat_scaled, image_data.iloc[:, -2:], likes_data], axis=1, sort=False)\n",
    "\n",
    "    # DataFrame of training set labels\n",
    "    train_labels = preprocess_labels(data_dir, sub_ids)\n",
    "\n",
    "    #return train_features, features_min_max, image_means, likes_kept, train_labels\n",
    "    return train_features, features_q10_q90, image_means, likes_kept, train_labels\n",
    "\n",
    "\n",
    "#def preprocess_test(data_dir, min_max_train, image_means_train, likes_kept_train):\n",
    "def preprocess_test(data_dir, q10_q90_train, image_means_train, likes_kept_train, scaling=True):\n",
    "    '''\n",
    "    Purpose: preprocesses test dataset (no labels)\n",
    "    Input:\n",
    "        datadir {string}: path to Test data directory\n",
    "        (**updated)q10_q90_train {tupple of two numpy arrays}: 10th and 90th quantile values for\n",
    "                concatenated text and image features (from train set)\n",
    "        image_means_train {list of float}: means from oxford training dataset to replace\n",
    "                missing entries in oxford test set\n",
    "        likes_kept_train {list of strings}: most frequent likes_ids from train set\n",
    "                (ordered by frequency) to serve as columns in relation features matrix\n",
    "        scaling {boolean}: if True, Robust scaling applied to data; no scaling if False       \n",
    "    Output:\n",
    "        test_features {pandas DataFrame}: vectorized features of test set\n",
    "    '''\n",
    "    # sub_ids: a numpy array of subject ids ordered alphabetically.\n",
    "    # text_data: a pandas DataFrame of unscaled text data (liwc and nrc)\n",
    "    sub_ids, text_data = get_text_data(data_dir)\n",
    "\n",
    "    # image_data: pandas dataframe of oxford data\n",
    "    # image_min_max: a tupple of 2 pandas series, the min and max values from oxford training features\n",
    "    image_data_raw = get_image_raw(data_dir)\n",
    "    image_data = get_image_clean(sub_ids, image_data_raw, image_means_train)\n",
    "\n",
    "    '''\n",
    "    Note: Scale the text and image data BEFORE concatenating with relations\n",
    "    '''\n",
    "    features_to_scale = pd.concat([text_data, image_data.iloc[:, :-2]], axis=1, sort=False)\n",
    "    #feat_min = min_max_train[0]\n",
    "    #feat_max = min_max_train[1]\n",
    "    feat_q10 = q10_q90_train[0]\n",
    "    feat_q90 = q10_q90_train[1]\n",
    "\n",
    "    #feat_scaled = (features_to_scale - feat_min) / (feat_max - feat_min)\n",
    "    if scaling:\n",
    "        feat_scaled = (features_to_scale - feat_q10) / (feat_q90 - feat_q10)\n",
    "    else:\n",
    "        feat_scaled = features_to_scale\n",
    "    \n",
    "    # multi-hot matrix of likes from train data\n",
    "    likes_data = get_relations(data_dir, sub_ids, likes_kept_train)\n",
    "\n",
    "    # concatenate all scaled features into a single DataFrame\n",
    "    test_features = pd.concat([feat_scaled, image_data.iloc[:, -2:], likes_data], axis=1, sort=False)\n",
    "\n",
    "    return test_features\n",
    "\n",
    "\n",
    "def get_train_val_sets(features, labels, val_prop):\n",
    "    '''\n",
    "    Purpose: Splits training dataset into a train and a validation set of\n",
    "    ratio determined by val_prop (x = features, y = labels)\n",
    "    Input\n",
    "        features {pandas DataFrame}: vectorized features scaled between 0 and 1\n",
    "                for each user id in the training set, concatenated for all modalities\n",
    "                (order = text + image + relation), with userid as DataFrame index.\n",
    "        labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "        val_prop {float between 0 and 1}: proportion of sample in validation set\n",
    "                    (e.g. 0.2 = 20% validation, 80% training)\n",
    "    Output:\n",
    "        x_train, x_val {pandas DataFrames}: vectorized features for train and validation sets\n",
    "        y_train, y_val {pandas DataFrames}: train and validation set labels\n",
    "\n",
    "    TO DO: convert outputted pandas to tensorflow tf.data.Dataset?...\n",
    "    https://www.tensorflow.org/guide/data\n",
    "    '''\n",
    "    # NOTE: UNUSED\n",
    "    from sklearn import model_selection\n",
    "    x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "        features, # training features to split\n",
    "        labels, # training labels to split\n",
    "        test_size = val_prop, # between 0 and 1, proportion of sample in validation set (e.g., 0.2)\n",
    "        shuffle= True,\n",
    "        #stratify = y_data[:1],\n",
    "        # random_state = 42  # can use to always obtain the same train/validation split\n",
    "        )\n",
    "\n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../Train/Text/liwc.csv' does not exist: b'../Train/Text/liwc.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0cf45d9aee41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../Train'\u001b[0m \u001b[0;31m#modify if working from other directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_q10_q90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikes_kept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_likes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-21730bd7467e>\u001b[0m in \u001b[0;36mpreprocess_train\u001b[0;34m(data_dir, num_likes, scaling)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;31m# sub_ids: a numpy array of subject ids ordered alphabetically.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;31m# text_data: a pandas DataFrame of unscaled text data (liwc and nrc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0msub_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m     \u001b[0;31m# image_data: pandas dataframe of oxford data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;31m# image_min_max: a tupple of 2 pandas series, the min and max values from oxford training features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-21730bd7467e>\u001b[0m in \u001b[0;36mget_text_data\u001b[0;34m(input_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Load and sort text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mliwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Text/liwc.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mliwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mliwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/datascience/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/datascience/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/datascience/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/datascience/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/datascience/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../Train/Text/liwc.csv' does not exist: b'../Train/Text/liwc.csv'"
     ]
    }
   ],
   "source": [
    "# to preprocess the training dataset:\n",
    "# 1. set path to Train directory\n",
    "# 2. call preprocess_train\n",
    "\n",
    "train_path = '../Train' #modify if working from other directory\n",
    "\n",
    "train_features, features_q10_q90, image_means, likes_kept, train_labels = preprocess_train(train_path, num_likes=10_000, scaling=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data into training and validation sets\n",
    "\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "    train_features, # training features to split\n",
    "    train_labels, # training labels to split\n",
    "    test_size = 0.2, # between 0 and 1, proportion of sample in validation set (e.g., 0.2)\n",
    "    shuffle= True,\n",
    "    stratify = train_labels['gender']\n",
    "    # random_state = 42  # can use to always obtain the same train/validation split\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4bb581a442f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mila/teaching/user07'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
