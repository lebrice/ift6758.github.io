{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn import model_selection\n",
    "\n",
    "from typing import *\n",
    "from collections import Counter\n",
    "#from utils import DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_text_data(input_dir):\n",
    "    \"\"\"\n",
    "    Purpose: preprocess liwc and nrc\n",
    "    Input\n",
    "        input_dir {string} : path to input_directory (ex, \"~/Train\")\n",
    "    Output:\n",
    "        id_list {numpy array of strings}: array of user ids sorted alphabetically,\n",
    "                                        to determine order of features and labels DataFrames\n",
    "        text_data {pandas DataFrame of float}: unscaled text data (liwc and nrc combined)\n",
    "    \"\"\"\n",
    "    # Load and sort text data\n",
    "    liwc = pd.read_csv(os.path.join(input_dir, 'Text/liwc.csv'), sep = ',')\n",
    "    liwc = liwc.sort_values(by=['userId'])\n",
    "\n",
    "    nrc = pd.read_csv(os.path.join(input_dir, 'Text/nrc.csv'), sep = ',')\n",
    "    nrc = nrc.sort_values(by=['userId'])\n",
    "\n",
    "    # Build list of subject ids ordered alphabetically\n",
    "    # Check if same subject lists in both sorted DataFrames (liwc and nrc)\n",
    "    if np.array_equal(liwc['userId'], nrc['userId']):\n",
    "        id_list = liwc['userId'].to_numpy()\n",
    "    else:\n",
    "        raise Exception('userIds do not match between liwc and nrc data')\n",
    "\n",
    "    # merge liwc and nrc DataFrames using userId as index\n",
    "    liwc.set_index('userId', inplace=True)\n",
    "    nrc.set_index('userId', inplace=True)\n",
    "\n",
    "    text_data = pd.concat([liwc, nrc], axis=1, sort=False)\n",
    "\n",
    "    return id_list, text_data\n",
    "\n",
    "\n",
    "def get_image_clean(sub_ids, oxford, means):\n",
    "    '''\n",
    "    Purpose: preprocess oxford metrics derived from profile pictures (part 2)\n",
    "    Input:\n",
    "        sub_ids {numpy array of strings}: ordered list of userIDs\n",
    "        oxford {pandas DataFrame of floats}: unscaled oxford features of users with 1+ face\n",
    "        means {list of float}: mean values for each feature averaged from train set,\n",
    "                    to replace missing values for userids with no face (train and test set)\n",
    "    Output:\n",
    "        image_data {pandas DataFrame of float}: unscaled oxford image data\n",
    "                with mean values replacing missing entries\n",
    "    '''\n",
    "    # list of ids with at least one face on image: 7174 out of 9500 in train set\n",
    "    ox_list = np.sort(oxford['userId'].unique(), axis=None)\n",
    "    # list of ids in text_list who have no face metrics in oxford.csv (2326 in train set)\n",
    "    ox_noface = np.setdiff1d(sub_ids, ox_list)\n",
    "\n",
    "    # Create DataFrame for userids with no face (1 row per userid)\n",
    "    # values are mean metrics averaged from users with entries (training set)\n",
    "    ox_nf = pd.DataFrame(ox_noface, columns = ['userId'])\n",
    "    columns = oxford.columns[2:].tolist()\n",
    "    for column, mean in zip(columns, means):\n",
    "        ox_nf.insert(loc=ox_nf.shape[1], column=column, value=mean, allow_duplicates=True)\n",
    "    # insert column 'noface' = 1 if no face in image, else 0\n",
    "    ox_nf.insert(loc=ox_nf.shape[1], column='noface', value=1, allow_duplicates=True)\n",
    "    # insert column 'multiface' = 1 if many faces in image, else 0\n",
    "    ox_nf.insert(loc=ox_nf.shape[1], column='multiface', value=0, allow_duplicates=True)\n",
    "    ox_nf.set_index('userId', inplace=True)\n",
    "\n",
    "    # Format DataFrame from userids with 1+ face\n",
    "    # insert column 'noface' = 1 if no face in image, else 0\n",
    "    oxford.insert(loc=oxford.shape[1], column='noface', value=0, allow_duplicates=True)\n",
    "    # list userIds with multiple faces (714 in train set)\n",
    "    ox_multiples = oxford['userId'][oxford['userId'].duplicated()].tolist()\n",
    "    # insert column 'multiface' = 1 if many faces in image, else 0\n",
    "    oxford.insert(loc=oxford.shape[1], column='multiface', value=0, allow_duplicates=True)\n",
    "    multi_mask = pd.Series([uid in ox_multiples for uid in oxford['userId']])\n",
    "    i = oxford[multi_mask].index\n",
    "    oxford.loc[i, 'multiface'] = 1\n",
    "    # drop duplicate entries with same userId (keep first entry per userId)\n",
    "    oxford.drop_duplicates(subset ='userId', keep='first', inplace=True)\n",
    "\n",
    "    # merge the two DataFrames\n",
    "    oxford.drop(['faceID'], axis=1, inplace=True)\n",
    "    oxford.set_index('userId', inplace=True)\n",
    "    image_data = pd.concat([ox_nf, oxford], axis=0, sort=False).sort_values(by=['userId'])\n",
    "\n",
    "    if not np.array_equal(image_data.index, sub_ids):\n",
    "        raise Exception('userIds do not match between oxford file and id list')\n",
    "\n",
    "    return image_data\n",
    "\n",
    "\n",
    "def get_image_raw(data_dir):\n",
    "    '''\n",
    "    Purpose: preprocess oxford metrics derived from profile pictures (part 1)\n",
    "    Input\n",
    "        input_dir {string} : path to input_directory (ex, \"~/Train\")\n",
    "    Output:\n",
    "        image_data {pandas DataFrame of float}: unscaled oxford image data\n",
    "    '''\n",
    "    # Load data of oxford features extracted from profile picture (face metrics)\n",
    "    # 7915 entries; some users have no face, some have multiple faces on image.\n",
    "    # userids with 1+ face on image: 7174 out of 9500 (train set)\n",
    "    # duplicated entries (userids with > 1 face on same image): 741 in train set\n",
    "    oxford = pd.read_csv(os.path.join(data_dir, \"Image\", \"oxford.csv\"), sep = ',')\n",
    "    #oxford = oxford.sort_values(by=['userId'])\n",
    "    '''\n",
    "    NOTE: headPose_pitch has NO RANGE, drop that feature\n",
    "    '''\n",
    "    oxford.drop(['headPose_pitch'], axis=1, inplace=True)\n",
    "\n",
    "    return oxford\n",
    "\n",
    "\n",
    "def get_likes_kept(data_dir, num_features) -> List[str]:\n",
    "    '''\n",
    "    Purpose: get list of likes to keep as features\n",
    "    Input:\n",
    "        data_dir {str} : the parent input directory\n",
    "        num_features {int} : the number of likes to keep as features,\n",
    "                        starting from those with highest frequencies\n",
    "    Output:\n",
    "        freq_like_id {List of strings}: frequency of most frequent likes,\n",
    "                    (number = num_features), in descending ordered, indexed by like_id\n",
    "    '''\n",
    "    #Why return frequency?\n",
    "    relation = pd.read_csv(os.path.join(data_dir, \"Relation\", \"Relation.csv\")) #, index_col=1)\n",
    "    relation = relation.drop(['Unnamed: 0'], axis=1)\n",
    "    like_ids_to_keep = relation['like_id'].value_counts(sort=True, ascending=False)[:num_features] #This sorts features by frequency\n",
    "\n",
    "    #sort like indices (which are the keys associated with the values kepts)\n",
    "    likes_int64_list = sorted(like_ids_to_keep.keys()) # This sorts indices by like_id\n",
    "    likes_str_list = [str(l) for l in likes_int64_list]\n",
    "    return likes_str_list\n",
    "\n",
    "\n",
    "def get_relations(data_dir: str, sub_ids: List[str], like_ids_to_keep: List[str]):\n",
    "    '''\n",
    "    Purpose: preprocess relations dataset ('likes')\n",
    "\n",
    "    Input:\n",
    "        data_dir {str} -- the parent input directory\n",
    "        sub_ids {numpy array of strings} -- the ordered list of userids\n",
    "        like_ids_to_keep {List[str]} -- The list of page IDs to keep.\n",
    "\n",
    "    Returns:\n",
    "        relations_data -- multihot matrix of the like_id. Rows are indexed with userid, entries are boolean.\n",
    "    '''\n",
    "    relation = pd.read_csv(os.path.join(data_dir, \"Relation\", \"Relation.csv\")) #, index_col=1)\n",
    "    relation = relation.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    ## One HUGE step:\n",
    "    # likes_to_keep = like_ids_to_keep.keys()\n",
    "    # kept_relations = relation[relation.like_id.isin(likes_to_keep)]\n",
    "    # multi_hot_relations = pd.get_dummies(kept_relations, columns=[\"like_id\"], prefix=\"\")\n",
    "    # multi_hot = multi_hot_relations.groupby((\"userid\")).sum()\n",
    "    # return multi_hot_relations\n",
    "    ###\n",
    "    total_num_pages = len(like_ids_to_keep)\n",
    "    # Create a multihot likes matrix of booleans (rows = userids, cols = likes), by batch\n",
    "    batch_size = 1000\n",
    "\n",
    "    # Create empty DataFrame with sub_ids as index list\n",
    "    relation_data = pd.DataFrame(sub_ids, columns = ['userid'])\n",
    "    relation_data.set_index('userid', inplace=True)\n",
    "\n",
    "    for start_index in range(0, total_num_pages, batch_size):\n",
    "        end_index = min(start_index + batch_size, total_num_pages)\n",
    "\n",
    "        # sets are better for membership testing than lists.\n",
    "        like_ids_for_this_batch = set(like_ids_to_keep[start_index:end_index])\n",
    "\n",
    "        filtered_table = relation[relation['like_id'].isin(like_ids_for_this_batch)]\n",
    "        ## THIS is the slow part:\n",
    "        relHot = pd.get_dummies(filtered_table, columns=['like_id'], prefix=\"\", prefix_sep=\"\")\n",
    "        ##\n",
    "        relHot = relHot.groupby(['userid']).sum().astype(float) # this makes userid the index\n",
    "\n",
    "        relation_data = pd.concat([relation_data, relHot], axis=1, sort=True)\n",
    "\n",
    "    relation_data = relation_data.reindex(like_ids_to_keep, axis=1)\n",
    "    relation_data.fillna(0.0, inplace=True)\n",
    "    relation_data = relation_data.astype(\"bool\")\n",
    "\n",
    "    # will be different if users in relation.csv are not in sub_ids\n",
    "    if not np.array_equal(relation_data.index, sub_ids):\n",
    "        raise Exception(f\"\"\"userIds do not match between relation file and id list:\n",
    "    {relation_data.index}\n",
    "    {sub_ids}\n",
    "\n",
    "    \"\"\")\n",
    "\n",
    "    return relation_data\n",
    "\n",
    "\n",
    "def make_label_dict(labels):\n",
    "    '''\n",
    "    Purpose: make dictionnary of labels from pandas DataFrame\n",
    "    Input:\n",
    "        labels {pandas DataFrame}: labels ordered per userids (alphabetical order)\n",
    "    Output:\n",
    "        labels_dict {dictionary of pandas DataFrames}: labels (one entry per metric) ordered alphabetically\n",
    "                by userid for the training set, with userids as index.\n",
    "\n",
    "    '''\n",
    "    gender = labels['gender']\n",
    "\n",
    "    age_grps = labels[['age_xx_24', 'age_25_34', 'age_35_49', 'age_50_xx']]\n",
    "\n",
    "    '''\n",
    "    Note: : each DataFrames (value) is indexed by userid in labels_dict\n",
    "    '''\n",
    "    labels_dict = {}\n",
    "    labels_dict['userid'] = labels.index\n",
    "    labels_dict['gender'] = gender\n",
    "    labels_dict['age_grps'] = age_grps\n",
    "    labels_dict['ope'] = labels['ope']\n",
    "    labels_dict['con'] = labels['con']\n",
    "    labels_dict['ext'] = labels['ext']\n",
    "    labels_dict['agr'] = labels['agr']\n",
    "    labels_dict['neu'] = labels['neu']\n",
    "\n",
    "    return labels_dict\n",
    "\n",
    "\n",
    "def preprocess_labels(data_dir, sub_ids):\n",
    "    '''\n",
    "    Purpose: preprocess entry labels from training set\n",
    "    Input:\n",
    "        datadir {string} : path to training data directory\n",
    "        sub_ids {numpy array of strings}: list of subject ids ordered alphabetically\n",
    "    Output:\n",
    "        labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "\n",
    "    '''\n",
    "    labels = pd.read_csv(os.path.join(data_dir, \"Profile\", \"Profile.csv\"))\n",
    "\n",
    "    def age_group_id(age_str: str) -> int:\n",
    "        \"\"\"Returns the age group category ID (an integer from 0 to 3) for the given age (string)\n",
    "\n",
    "        Arguments:\n",
    "            age_str {str} -- the age\n",
    "\n",
    "        Returns:\n",
    "            int -- the ID of the age group: 0 for xx-24, 1 for 25-34, 2 for 35-49 and 3 for 50-xx.\n",
    "        \"\"\"\n",
    "        age = int(age_str)\n",
    "        if age <= 24:\n",
    "            return 0\n",
    "        elif age <= 34:\n",
    "            return 1\n",
    "        elif age <= 49:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "    \n",
    "    labels = labels.assign(age_group = lambda dt: pd.Series([age_group_id(age_str) for age_str in dt[\"age\"]]))\n",
    "    #labels = labels.assign(age_xx_24 = lambda dt: pd.Series([int(age) <= 24 for age in dt[\"age\"]]))\n",
    "    #labels = labels.assign(age_25_34 = lambda dt: pd.Series([25 <= int(age) <= 34 for age in dt[\"age\"]]))\n",
    "    #labels = labels.assign(age_35_49 = lambda dt: pd.Series([35 <= int(age) <= 49 for age in dt[\"age\"]]))\n",
    "    #labels = labels.assign(age_50_xx = lambda dt: pd.Series([50 <= int(age) for age in dt[\"age\"]]))\n",
    "\n",
    "    labels = labels.sort_values(by=['userid'])\n",
    "    \n",
    "    # check if same subject ids in labels and sub_ids\n",
    "    if not np.array_equal(labels['userid'].to_numpy(), sub_ids):\n",
    "        raise Exception('userIds do not match between profiles labels and id list')\n",
    "            \n",
    "    labels.set_index('userid', inplace=True)\n",
    "    \n",
    "    labels = labels.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "    return labels\n",
    "\n",
    "def preprocess_train(data_dir, num_likes=10_000):\n",
    "    '''\n",
    "    Purpose: preprocesses training dataset (with labels) and returns scaled features,\n",
    "    labels and parameters to scale the test data set\n",
    "    Input\n",
    "        data_dir {string}: path to ~/Train data directory\n",
    "        num_likes {int}: number of like_ids to keep as features\n",
    "    Output:\n",
    "        train_features {pandas DataFrame}: vectorized features scaled between 0 and 1\n",
    "                for each user id in the training set, concatenated for all modalities\n",
    "                (order = text + image + relation), with userid as DataFrame index.\n",
    "        **(updated:)features_q10_q90 {tupple of 2 pandas Series}: series of 10th and 90th quantile values of\n",
    "                text + image features from train dataset, to be used to scale test data.\n",
    "                Note that the multihot relation features do not necessitate scaling.\n",
    "        image_means {list of float}: means from oxford dataset to replace missing entries in oxford test set\n",
    "        likes_kept {list of strings}: ordered likes_ids to serve as columns for test set relation features matrix\n",
    "        train_labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "\n",
    "    TO CONSIDER: convert outputted pandas to tensorflow tf.data.Dataset...\n",
    "    https://www.tensorflow.org/guide/data\n",
    "    '''\n",
    "    # sub_ids: a numpy array of subject ids ordered alphabetically.\n",
    "    # text_data: a pandas DataFrame of unscaled text data (liwc and nrc)\n",
    "    sub_ids, text_data = get_text_data(data_dir)\n",
    "    # image_data: pandas dataframe of oxford data\n",
    "    # image_min_max: a tupple of 2 pandas series, the min and max values from oxford training features\n",
    "    image_data_raw = get_image_raw(data_dir)\n",
    "    image_means = image_data_raw.iloc[:, 2:].mean().tolist()\n",
    "    image_data = get_image_clean(sub_ids, image_data_raw, image_means)\n",
    "\n",
    "    '''\n",
    "    Note: Scale the text and image data BEFORE concatenating with relations\n",
    "    Update: scaling w RobustScaler rather than MinMaxScaler algo, due to outliers\n",
    "    '''\n",
    "    features_to_scale = pd.concat([text_data, image_data.iloc[:, :-2]], axis=1, sort=False)\n",
    "    #feat_min = features_to_scale.min()\n",
    "    #feat_max = features_to_scale.max()\n",
    "    feat_q10 = features_to_scale.quantile(q = 0.10)\n",
    "    feat_q90 = features_to_scale.quantile(q = 0.90)\n",
    "\n",
    "    #feat_scaled = (features_to_scale - feat_min) / (feat_max - feat_min)\n",
    "    #features_min_max = (feat_min, feat_max)\n",
    "    feat_scaled = (features_to_scale - feat_q10) / (feat_q90 - feat_q10)\n",
    "    features_q10_q90 = (feat_q10, feat_q90)\n",
    "\n",
    "    #if DEBUG:\n",
    "    #    likes_kept = [str(v) for v in range(num_likes)]\n",
    "    #else:\n",
    "    likes_kept = get_likes_kept(data_dir, num_likes)\n",
    "\n",
    "    # multi-hot matrix of likes from train data\n",
    "    likes_data = get_relations(data_dir, sub_ids, likes_kept)\n",
    "\n",
    "    # concatenate all scaled features into a single DataFrame\n",
    "    train_features = pd.concat([feat_scaled, image_data.iloc[:, -2:], likes_data], axis=1, sort=False)\n",
    "\n",
    "    # DataFrame of training set labels\n",
    "    train_labels = preprocess_labels(data_dir, sub_ids)\n",
    "\n",
    "    #return train_features, features_min_max, image_means, likes_kept, train_labels\n",
    "    return train_features, features_q10_q90, image_means, likes_kept, train_labels\n",
    "\n",
    "\n",
    "#def preprocess_test(data_dir, min_max_train, image_means_train, likes_kept_train):\n",
    "def preprocess_test(data_dir, q10_q90_train, image_means_train, likes_kept_train):\n",
    "    '''\n",
    "    Purpose: preprocesses test dataset (no labels)\n",
    "    Input:\n",
    "        datadir {string}: path to Test data directory\n",
    "        (**updated)q10_q90_train {tupple of two numpy arrays}: 10th and 90th quantile values for\n",
    "                concatenated text and image features (from train set)\n",
    "        image_means_train {list of float}: means from oxford training dataset to replace\n",
    "                missing entries in oxford test set\n",
    "        likes_kept_train {list of strings}: most frequent likes_ids from train set\n",
    "                (ordered by frequency) to serve as columns in relation features matrix\n",
    "    Output:\n",
    "        test_features {pandas DataFrame}: vectorized features of test set\n",
    "    '''\n",
    "    # sub_ids: a numpy array of subject ids ordered alphabetically.\n",
    "    # text_data: a pandas DataFrame of unscaled text data (liwc and nrc)\n",
    "    sub_ids, text_data = get_text_data(data_dir)\n",
    "\n",
    "    # image_data: pandas dataframe of oxford data\n",
    "    # image_min_max: a tupple of 2 pandas series, the min and max values from oxford training features\n",
    "    image_data_raw = get_image_raw(data_dir)\n",
    "    image_data = get_image_clean(sub_ids, image_data_raw, image_means_train)\n",
    "\n",
    "    '''\n",
    "    Note: Scale the text and image data BEFORE concatenating with relations\n",
    "    '''\n",
    "    features_to_scale = pd.concat([text_data, image_data.iloc[:, :-2]], axis=1, sort=False)\n",
    "    #feat_min = min_max_train[0]\n",
    "    #feat_max = min_max_train[1]\n",
    "    feat_q10 = q10_q90_train[0]\n",
    "    feat_q90 = q10_q90_train[1]\n",
    "\n",
    "    #feat_scaled = (features_to_scale - feat_min) / (feat_max - feat_min)\n",
    "    feat_scaled = (features_to_scale - feat_q10) / (feat_q90 - feat_q10)\n",
    "\n",
    "    # multi-hot matrix of likes from train data\n",
    "    likes_data = get_relations(data_dir, sub_ids, likes_kept_train)\n",
    "\n",
    "    # concatenate all scaled features into a single DataFrame\n",
    "    test_features = pd.concat([feat_scaled, image_data.iloc[:, -2:], likes_data], axis=1, sort=False)\n",
    "\n",
    "    return test_features\n",
    "\n",
    "\n",
    "def get_train_val_sets(features, labels, val_prop):\n",
    "    '''\n",
    "    Purpose: Splits training dataset into a train and a validation set of\n",
    "    ratio determined by val_prop (x = features, y = labels)\n",
    "    Input\n",
    "        features {pandas DataFrame}: vectorized features scaled between 0 and 1\n",
    "                for each user id in the training set, concatenated for all modalities\n",
    "                (order = text + image + relation), with userid as DataFrame index.\n",
    "        labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "        val_prop {float between 0 and 1}: proportion of sample in validation set\n",
    "                    (e.g. 0.2 = 20% validation, 80% training)\n",
    "    Output:\n",
    "        x_train, x_val {pandas DataFrames}: vectorized features for train and validation sets\n",
    "        y_train, y_val {pandas DataFrames}: train and validation set labels\n",
    "\n",
    "    TO DO: convert outputted pandas to tensorflow tf.data.Dataset?...\n",
    "    https://www.tensorflow.org/guide/data\n",
    "    '''\n",
    "    # NOTE: UNUSED\n",
    "    from sklearn import model_selection\n",
    "    x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "        features, # training features to split\n",
    "        labels, # training labels to split\n",
    "        test_size = val_prop, # between 0 and 1, proportion of sample in validation set (e.g., 0.2)\n",
    "        shuffle= True,\n",
    "        #stratify = y_data[:1],\n",
    "        # random_state = 42  # can use to always obtain the same train/validation split\n",
    "        )\n",
    "\n",
    "    return x_train, x_val, y_train, y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, features_q10_q90, image_means, likes_kept, train_labels = preprocess_train('../Train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_ids(input_dir):\n",
    "    \"\"\"\n",
    "    Purpose: preprocess liwc and nrc\n",
    "    Input\n",
    "        input_dir {string} : path to input_directory (ex, \"~/Train\")\n",
    "    Output:\n",
    "        id_list {numpy array of strings}: array of user ids sorted alphabetically,\n",
    "                                        to determine order of features and labels DataFrames\n",
    "        text_data {pandas DataFrame of float}: unscaled text data (liwc and nrc combined)\n",
    "    \"\"\"\n",
    "    # Load and sort text data\n",
    "    liwc = pd.read_csv(os.path.join(input_dir, 'Text/liwc.csv'), sep = ',')\n",
    "    liwc = liwc.sort_values(by=['userId'])\n",
    "\n",
    "    nrc = pd.read_csv(os.path.join(input_dir, 'Text/nrc.csv'), sep = ',')\n",
    "    nrc = nrc.sort_values(by=['userId'])\n",
    "\n",
    "    # Build list of subject ids ordered alphabetically\n",
    "    # Check if same subject lists in both sorted DataFrames (liwc and nrc)\n",
    "    if np.array_equal(liwc['userId'], nrc['userId']):\n",
    "        id_list = liwc['userId'].to_numpy()\n",
    "    else:\n",
    "        raise Exception('userIds do not match between liwc and nrc data')\n",
    "\n",
    "    return id_list\n",
    "\n",
    "def preprocess_labels(data_dir, sub_ids):\n",
    "    '''\n",
    "    Purpose: preprocess entry labels from training set\n",
    "    Input:\n",
    "        datadir {string} : path to training data directory\n",
    "        sub_ids {numpy array of strings}: list of subject ids ordered alphabetically\n",
    "    Output:\n",
    "        labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "\n",
    "    '''\n",
    "    labels = pd.read_csv(os.path.join(data_dir, \"Profile\", \"Profile.csv\"))\n",
    "\n",
    "    def age_group_id(age_str: str) -> int:\n",
    "        \"\"\"Returns the age group category ID (an integer from 0 to 3) for the given age (string)\n",
    "\n",
    "        Arguments:\n",
    "            age_str {str} -- the age\n",
    "\n",
    "        Returns:\n",
    "            int -- the ID of the age group: 0 for xx-24, 1 for 25-34, 2 for 35-49 and 3 for 50-xx.\n",
    "        \"\"\"\n",
    "        age = int(age_str)\n",
    "        if age <= 24:\n",
    "            return 0\n",
    "        elif age <= 34:\n",
    "            return 1\n",
    "        elif age <= 49:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    labels = labels.assign(age_group = lambda dt: pd.Series([age_group_id(age_str) for age_str in dt[\"age\"]]))\n",
    "\n",
    "    labels = labels.sort_values(by=['userid'])\n",
    "    # check if same subject ids in labels and sub_ids\n",
    "    if not np.array_equal(labels['userid'].to_numpy(), sub_ids):\n",
    "        raise Exception('userIds do not match between profiles labels and id list')    \n",
    "    \n",
    "    labels = labels.drop(['Unnamed: 0'], axis=1)\n",
    "    labels.set_index('userid', inplace=True)\n",
    "\n",
    "    labels_train, labels_val = model_selection.train_test_split(\n",
    "        labels, # training features to split\n",
    "        test_size = 0.2, # between 0 and 1, proportion of sample in validation set (e.g., 0.2)\n",
    "        shuffle= True)    \n",
    "    \n",
    "    return labels_train, labels_val\n",
    "\n",
    "def extract_likes(data_dir, labels_train, labels_val, num_features, keepall=True):\n",
    "    '''\n",
    "    Purpose: get list of likes to keep as features\n",
    "    Input:\n",
    "        data_dir {str} : the parent input directory\n",
    "        num_features {int} : the number of likes to keep as features,\n",
    "                        starting from those with highest frequencies\n",
    "        keepall {boolean}: if True, keep all likes, else keep num_features (ranked by frequency)                \n",
    "    Output:\n",
    "        freq_like_id {List of strings}: frequency of most frequent likes,\n",
    "                    (number = num_features), in descending ordered, indexed by like_id\n",
    "    '''\n",
    "    train_ids = labels_train.index.values.tolist() # list of userids in train set\n",
    "    val_ids = labels_val.index.values.tolist() # list of userids in validation set\n",
    "    \n",
    "    relation = pd.read_csv(os.path.join(data_dir, \"Relation\", \"Relation.csv\")) #, index_col=1)\n",
    "    relation = relation.drop(['Unnamed: 0'], axis=1)\n",
    "    if keepall:\n",
    "        like_ids_to_keep = relation['like_id'].value_counts(sort=True, ascending=False) # sorts features by frequency        \n",
    "    else:    \n",
    "        like_ids_to_keep = relation['like_id'].value_counts(sort=True, ascending=False)[:num_features] # sorts features by frequency\n",
    "    \n",
    "    likes_dict = {}\n",
    "    val_data = {}\n",
    "    \n",
    "    for i in relation.index:\n",
    "        userid = relation.loc[i, 'userid']\n",
    "        like_id = relation.loc[i, 'like_id']\n",
    "        if userid in train_ids:\n",
    "            if like_id in like_ids_to_keep:\n",
    "                if like_id not in likes_dict:\n",
    "                    likes_dict[like_id] = np.zeros(6)\n",
    "                likes_dict[like_id][0] += labels_train.loc[userid, 'ope']\n",
    "                likes_dict[like_id][1] += labels_train.loc[userid, 'con']\n",
    "                likes_dict[like_id][2] += labels_train.loc[userid, 'ext']\n",
    "                likes_dict[like_id][3] += labels_train.loc[userid, 'agr']\n",
    "                likes_dict[like_id][4] += labels_train.loc[userid, 'neu']\n",
    "                likes_dict[like_id][5] += 1                \n",
    "        elif userid in val_ids:\n",
    "            if userid not in val_data:\n",
    "                val_data[userid] = [] # empty list of strings (like_ids)\n",
    "            val_data[userid].append(like_id)\n",
    "\n",
    "    return likes_dict, val_data\n",
    "\n",
    "def get_val_predictions(likes_dict, val_likes, labels_val):\n",
    "    predicted_vals = pd.DataFrame(labels_val)\n",
    "    colNames = ['pred_ope', 'pred_con', 'pred_ext', 'pred_agr', 'pred_neu']\n",
    "    means = labels_val.mean()\n",
    "    default_vals = [means['ope'], means['con'], means['ext'], means['agr'], means['neu']]\n",
    "\n",
    "    for j in range(5):\n",
    "        predicted_vals.insert(loc=predicted_vals.shape[1], column=colNames[j], \n",
    "                              value=default_vals[j], allow_duplicates=True)\n",
    "    \n",
    "    #big5 = ['ope', 'con', 'ext', 'agr', 'neu']\n",
    "    \n",
    "    for index in predicted_vals.index: #note that index is userid, key to val_likes dictionary\n",
    "        my_likes = val_likes[index]\n",
    "        my_pred_scores = np.zeros(5)\n",
    "        \n",
    "        for like in my_likes:\n",
    "            if like in likes_dict:\n",
    "                like_vals = likes_dict[like]\n",
    "                big5_vals = like_vals[:5]/like_vals[5]\n",
    "                my_pred_scores += big5_vals\n",
    "        \n",
    "        my_pred_scores = my_pred_scores/len(my_likes)\n",
    "        \n",
    "        predicted_vals.loc[index, 'pred_ope'] = my_pred_scores[0]\n",
    "        predicted_vals.loc[index, 'pred_con'] = my_pred_scores[1]        \n",
    "        predicted_vals.loc[index, 'pred_ext'] = my_pred_scores[2]  \n",
    "        predicted_vals.loc[index, 'pred_agr'] = my_pred_scores[3]\n",
    "        predicted_vals.loc[index, 'pred_neu'] = my_pred_scores[4]\n",
    "    \n",
    "    return predicted_vals\n",
    "    \n",
    "def get_validation_score(data_dir, num_likes=10_000):\n",
    "    '''\n",
    "    Purpose: preprocesses training dataset (with labels) and returns scaled features,\n",
    "    labels and parameters to scale the test data set\n",
    "    Input\n",
    "        data_dir {string}: path to ~/Train data directory\n",
    "        num_likes {int}: number of like_ids to keep as features\n",
    "    Output:\n",
    "        train_features {pandas DataFrame}: vectorized features scaled between 0 and 1\n",
    "                for each user id in the training set, concatenated for all modalities\n",
    "                (order = text + image + relation), with userid as DataFrame index.\n",
    "        **(updated:)features_q10_q90 {tupple of 2 pandas Series}: series of 10th and 90th quantile values of\n",
    "                text + image features from train dataset, to be used to scale test data.\n",
    "                Note that the multihot relation features do not necessitate scaling.\n",
    "        image_means {list of float}: means from oxford dataset to replace missing entries in oxford test set\n",
    "        likes_kept {list of strings}: ordered likes_ids to serve as columns for test set relation features matrix\n",
    "        train_labels {pandas DataFrame}: labels ordered by userid (alphabetically)\n",
    "                for the training set, with userids as index.\n",
    "\n",
    "    TO CONSIDER: convert outputted pandas to tensorflow tf.data.Dataset...\n",
    "    https://www.tensorflow.org/guide/data\n",
    "    '''\n",
    "    # sub_ids: a numpy array of subject ids ordered alphabetically (from text data).\n",
    "    sub_ids = get_sub_ids(data_dir)\n",
    "\n",
    "    # DataFrame of training set labels\n",
    "    labels_train, labels_val = preprocess_labels(data_dir, sub_ids)\n",
    "    \n",
    "    # building dictionnary, equivalent to algo/classifier training\n",
    "    likes_dict, val_likes = extract_likes(data_dir, labels_train, labels_val, num_likes, keepall=True)\n",
    "    \n",
    "    # predict validation data\n",
    "    val_predictions = get_val_predictions(likes_dict, val_likes, labels_val)\n",
    "    \n",
    "    return val_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../Train'\n",
    "validation_predictions = get_validation_score(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>ope</th>\n",
       "      <th>con</th>\n",
       "      <th>ext</th>\n",
       "      <th>agr</th>\n",
       "      <th>neu</th>\n",
       "      <th>age_group</th>\n",
       "      <th>pred_ope</th>\n",
       "      <th>pred_con</th>\n",
       "      <th>pred_ext</th>\n",
       "      <th>pred_agr</th>\n",
       "      <th>pred_neu</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1cdf6bd142450583278492aae4eff7bc</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.25</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3.177125</td>\n",
       "      <td>2.547758</td>\n",
       "      <td>2.660662</td>\n",
       "      <td>2.716805</td>\n",
       "      <td>2.150965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5262183f462a8917893a60343b15f982</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0</td>\n",
       "      <td>3.335828</td>\n",
       "      <td>2.806485</td>\n",
       "      <td>3.040893</td>\n",
       "      <td>3.072382</td>\n",
       "      <td>2.432452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98320d0b8f299d584e8f881c42dc27cf</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.40</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0</td>\n",
       "      <td>1.896751</td>\n",
       "      <td>1.716459</td>\n",
       "      <td>1.822927</td>\n",
       "      <td>1.677651</td>\n",
       "      <td>1.389482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9d781fc13f70dd59dcae5a2773e3b83d</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.80</td>\n",
       "      <td>2.35</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.25</td>\n",
       "      <td>3.15</td>\n",
       "      <td>2</td>\n",
       "      <td>2.629239</td>\n",
       "      <td>2.316536</td>\n",
       "      <td>2.323885</td>\n",
       "      <td>2.366685</td>\n",
       "      <td>1.845160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73b8a1a7f2556d4d08bdbb8f6418087f</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.30</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1</td>\n",
       "      <td>1.544444</td>\n",
       "      <td>1.343778</td>\n",
       "      <td>1.594667</td>\n",
       "      <td>1.617500</td>\n",
       "      <td>0.888667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8c26b76f1a820be3337414c5322aafee</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.05</td>\n",
       "      <td>2.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>028382aaaca3b1dc1eff78e71602ea0f</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.42</td>\n",
       "      <td>3.83</td>\n",
       "      <td>2.42</td>\n",
       "      <td>0</td>\n",
       "      <td>2.203174</td>\n",
       "      <td>1.868618</td>\n",
       "      <td>1.900576</td>\n",
       "      <td>2.051897</td>\n",
       "      <td>1.559377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ae1593fa350ba36f4178c25d74cc05f9</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0</td>\n",
       "      <td>2.209182</td>\n",
       "      <td>1.819818</td>\n",
       "      <td>1.894636</td>\n",
       "      <td>2.028030</td>\n",
       "      <td>1.548939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>bb97da8cce6cca8a13d4a583b720ccb4</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2.797515</td>\n",
       "      <td>2.358485</td>\n",
       "      <td>2.444729</td>\n",
       "      <td>2.472512</td>\n",
       "      <td>1.941950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7567b6224ec2181f7c1e9b160bc03a93</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.90</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.642923</td>\n",
       "      <td>0.503321</td>\n",
       "      <td>0.536196</td>\n",
       "      <td>0.544420</td>\n",
       "      <td>0.481316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1900 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   age  gender   ope   con   ext   agr   neu  \\\n",
       "userid                                                                         \n",
       "1cdf6bd142450583278492aae4eff7bc  27.0     1.0  2.75  2.00  3.75  2.25  4.00   \n",
       "5262183f462a8917893a60343b15f982  22.0     1.0  2.75  2.75  4.50  2.50  2.25   \n",
       "98320d0b8f299d584e8f881c42dc27cf  23.0     0.0  3.80  3.50  2.40  3.95  2.55   \n",
       "9d781fc13f70dd59dcae5a2773e3b83d  39.0     1.0  4.80  2.35  4.70  4.25  3.15   \n",
       "73b8a1a7f2556d4d08bdbb8f6418087f  31.0     0.0  4.30  3.20  3.80  3.65  2.50   \n",
       "...                                ...     ...   ...   ...   ...   ...   ...   \n",
       "8c26b76f1a820be3337414c5322aafee  24.0     0.0  4.20  3.60  3.70  4.05  2.10   \n",
       "028382aaaca3b1dc1eff78e71602ea0f  21.0     1.0  3.75  3.17  2.42  3.83  2.42   \n",
       "ae1593fa350ba36f4178c25d74cc05f9  23.0     1.0  2.95  3.85  3.50  3.75  2.05   \n",
       "bb97da8cce6cca8a13d4a583b720ccb4  26.0     0.0  4.00  2.75  3.25  4.33  3.00   \n",
       "7567b6224ec2181f7c1e9b160bc03a93  27.0     1.0  3.65  3.20  2.85  2.90  5.00   \n",
       "\n",
       "                                  age_group  pred_ope  pred_con  pred_ext  \\\n",
       "userid                                                                      \n",
       "1cdf6bd142450583278492aae4eff7bc          1  3.177125  2.547758  2.660662   \n",
       "5262183f462a8917893a60343b15f982          0  3.335828  2.806485  3.040893   \n",
       "98320d0b8f299d584e8f881c42dc27cf          0  1.896751  1.716459  1.822927   \n",
       "9d781fc13f70dd59dcae5a2773e3b83d          2  2.629239  2.316536  2.323885   \n",
       "73b8a1a7f2556d4d08bdbb8f6418087f          1  1.544444  1.343778  1.594667   \n",
       "...                                     ...       ...       ...       ...   \n",
       "8c26b76f1a820be3337414c5322aafee          0  0.000000  0.000000  0.000000   \n",
       "028382aaaca3b1dc1eff78e71602ea0f          0  2.203174  1.868618  1.900576   \n",
       "ae1593fa350ba36f4178c25d74cc05f9          0  2.209182  1.819818  1.894636   \n",
       "bb97da8cce6cca8a13d4a583b720ccb4          1  2.797515  2.358485  2.444729   \n",
       "7567b6224ec2181f7c1e9b160bc03a93          1  0.642923  0.503321  0.536196   \n",
       "\n",
       "                                  pred_agr  pred_neu  \n",
       "userid                                                \n",
       "1cdf6bd142450583278492aae4eff7bc  2.716805  2.150965  \n",
       "5262183f462a8917893a60343b15f982  3.072382  2.432452  \n",
       "98320d0b8f299d584e8f881c42dc27cf  1.677651  1.389482  \n",
       "9d781fc13f70dd59dcae5a2773e3b83d  2.366685  1.845160  \n",
       "73b8a1a7f2556d4d08bdbb8f6418087f  1.617500  0.888667  \n",
       "...                                    ...       ...  \n",
       "8c26b76f1a820be3337414c5322aafee  0.000000  0.000000  \n",
       "028382aaaca3b1dc1eff78e71602ea0f  2.051897  1.559377  \n",
       "ae1593fa350ba36f4178c25d74cc05f9  2.028030  1.548939  \n",
       "bb97da8cce6cca8a13d4a583b720ccb4  2.472512  1.941950  \n",
       "7567b6224ec2181f7c1e9b160bc03a93  0.544420  0.481316  \n",
       "\n",
       "[1900 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_predictions_plus = validation_predictions.copy()\n",
    "\n",
    "colNames = ['ope_boost', 'con_boost', 'ext_boost', 'agr_boost', 'neu_boost']\n",
    "\n",
    "means = validation_predictions_plus.mean()\n",
    "default_vals = [means['ope'], means['con'], means['ext'], means['agr'], means['neu']]\n",
    "\n",
    "for j in range(5):\n",
    "    validation_predictions_plus.insert(loc=validation_predictions_plus.shape[1], \n",
    "                                       column=colNames[j], value=default_vals[j], \n",
    "                                       allow_duplicates=True)\n",
    "\n",
    "boost_val = 0.1\n",
    "\n",
    "#big5 = ['ope', 'con', 'ext', 'agr', 'neu']\n",
    "    \n",
    "#note that index is userid, key to val_likes dictionary\n",
    "\n",
    "for index in validation_predictions_plus.index:     \n",
    "        \n",
    "    if validation_predictions_plus.loc[index, 'pred_ope'] >= means['ope']:\n",
    "        validation_predictions_plus.loc[index, 'ope_boost'] += boost_val\n",
    "    else:\n",
    "        validation_predictions_plus.loc[index, 'ope_boost'] -= boost_val\n",
    "         \n",
    "    if validation_predictions_plus.loc[index, 'pred_con'] >= means['con']:\n",
    "        validation_predictions_plus.loc[index, 'con_boost'] += boost_val\n",
    "    else:\n",
    "        validation_predictions_plus.loc[index, 'con_boost'] -= boost_val\n",
    "\n",
    "    if validation_predictions_plus.loc[index, 'pred_ext'] >= means['ext']:\n",
    "        validation_predictions_plus.loc[index, 'ext_boost'] += boost_val\n",
    "    else:\n",
    "        validation_predictions_plus.loc[index, 'ext_boost'] -= boost_val\n",
    "\n",
    "    if validation_predictions_plus.loc[index, 'pred_agr'] >= means['agr']:\n",
    "        validation_predictions_plus.loc[index, 'agr_boost'] += boost_val\n",
    "    else:\n",
    "        validation_predictions_plus.loc[index, 'agr_boost'] -= boost_val\n",
    "\n",
    "    if validation_predictions_plus.loc[index, 'pred_neu'] >= means['neu']:\n",
    "        validation_predictions_plus.loc[index, 'neu_boost'] += boost_val\n",
    "    else:\n",
    "        validation_predictions_plus.loc[index, 'neu_boost'] -= boost_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age          26.660000\n",
       "gender        0.570000\n",
       "ope           3.909453\n",
       "con           3.441989\n",
       "ext           3.486905\n",
       "agr           3.575395\n",
       "neu           2.718821\n",
       "age_group     0.606316\n",
       "pred_ope      2.565637\n",
       "pred_con      2.173582\n",
       "pred_ext      2.260909\n",
       "pred_agr      2.309010\n",
       "pred_neu      1.819222\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means\n",
    "#np.sum(validation_predictions_plus['pred_con'] >= means['con'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.74186268, 1.66228186, 1.6961997 , 1.6444449 , 1.34214659])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate rmse\n",
    "results = validation_predictions_plus\n",
    "\n",
    "y_vals = results.iloc[:, 2:7].values\n",
    "y_pred = results.iloc[:, 13:18].values\n",
    "#y_pred = results.iloc[:, 8:13].values\n",
    "means = results.iloc[:, 2:7].mean().values\n",
    "\n",
    "model_rmse = np.sqrt(np.mean((y_pred - y_vals)**2, axis=0))\n",
    "model_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62345047, 0.69995068, 0.78524895, 0.66708685, 0.77859712])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_rmse = np.sqrt(np.mean((y_vals - means)**2, axis=0))\n",
    "baseline_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
